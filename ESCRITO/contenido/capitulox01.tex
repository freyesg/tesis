% Deep Learning (Adaptive Computation and Machine Learning series)
\chapter{Introducción}
\section{Antecedentes y motivación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% lista3
El aprendizaje profundo ({\em Deep learning}, DL) se refiere a una nueva clase de métodos de las máquinas de aprendizaje ({\em Machine learning}, ML), donde el proceso de muchas capas distribuida en una arquitectura jerárquica se puede utilizar para clasificar un patrón y el aprendizaje de características \cite{Hinton2006, Bengio2009}. Esta arquitectura de aprendizaje jerárquico se inspira en la inteligencia artificial que emula el proceso de aprendizaje profundo y en capas de las áreas sensoriales primarias del neocórtex en el cerebro humano, que extrae automáticamente rasgos y abstracciones de los datos \cite{Bengio2007, Bengio2013, Arel2010}. %Esta técnica se encuentra en las intersecciones entre las áreas de investigación en el modelo gráfico, optimización, procesamiento de señales, reconocimiento de patrones y redes neuronales.
%Tres razones que explican la popularidad del DL es que aumenta drásticamente las capacidades de procesamiento de chips, reduce significativamente el costo de hardware de computación y los recientes avances en la investigación de ML [1].

En general, las técnicas del DL pueden clasificarse en modelos discriminativos profundos y modelos generativos \cite{Deng2014}. Ejemplos de modelos discriminativos son las redes neurales profundas ({\em Deep neural networks}, DNN), redes neuronales recurrentes ({\em Recurrent neural networks}, RNN) y redes neuronales convolucionales ({\em Convolutional neural networks}, CNN). Por otro lado, los modelos generativos, por ejemplo, son máquinas de Boltzmann restringidas ({\em Restricted Boltzmann machine}, RBMs), redes de creencias profundas ({\em Deep belief networks}, DBN), autocodificadores regularizados y máquinas profundas de Boltzmann (DBMs). %Entre estas técnicas, las CNN son el foco de este documento. CNNs es un profundo aprendizaje supervisado, mientras que este modelo suele ser eficiente para entrenar y probar, flexible para construir, y adecuado para el aprendizaje de extremo a extremo del sistema complejo \cite{Deng2014}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% lista4
En los últimos años, se han desarrollado una serie de investigaciones en base a los algoritmos del DL en varios campos diferentes. Reconocimiento automático de voz, reconocimiento de imágenes, procesamiento del lenguaje natural, descubrimiento y toxicología de medicamentos, gestión de relaciones con los clientes y bioinformática.

\paragrahp{Reconocimiento automático de voz} Google adopta las DNN para las búsquedas por voz como la tecnología principal que permite determinar que sonido es emitido en cada momentos, aumentando la rpecisión del reconocimiento de voz [8].

\paragraph{Reconocimiento de imagen} Las redes neuronales convolucionales de max-pooling profundo permiten detectar la mitosis en las imágenes de histología de mama [10].

\paragraph{Procesamiento natural del lenguaje} los métodos de DL se han aplicado con éxito a una variedad de idiomas y aplicaciones de recuperación de información. Al explotar arquitecturas profundas, las técnicas de DL son capaces de descubrir a partir de los datos de entrenamiento las estructuras y características ocultas en diferentes niveles de abstracciones útiles para cualquier tarea. En 2013, [12] propuso utilizar una DNN para clasificar un conjunto de documentos para una consulta determinada. Los resultados muestran que el modelo propuesto mejora significativamente a otros modelos semánticos [12].

\paragraph{Descubrimiento y Toxicología de Medicamentos} Los Estudios de Estructura Cuantitativa/Estudios de Predicción (QSAR/QSPR) intentan construir modelos matemáticos relacionando las propiedades físicas y químicas de los compuestos con su estructura química. En [13], el aprendizaje multitarea se aplica a QSAR utilizando diversos modelos de redes neuronales. Utilizaron una red neuronal artificial para aprender una función que predice actividades de compuestos para ensayos múltiples al mismo tiempo. El método se compara con métodos alternativos e informa que las redes neuronales con multitarea pueden conducir a resultados significativamente mejorados sobre líneas de base generadas con bosques aleatorios.

En 2015, AtomNet se ha introducido como primera estructura basada en la red neuronal convolucional profunda que diseñó para predecir la bioactividad de las moléculas pequeñas para aplicaciones de descubrimiento de fármacos [14]. Este artículo también demuestra cómo aplicar los conceptos convolucionales de localidad característica y composición jerárquica al modelado de la bioactividad y las interacciones químicas. AtomNet supera los criterios anteriores de acoplamiento en un conjunto diverso de puntos de referencia por un amplio margen, logrando un AUC mayor que 0,9 en el 57,8\% de los objetivos en el benchmark DUDE.



\paragraph{Gestión de relaciones con los clientes} Un marco para el control autónomo de un sistema de gestión de la relación con el cliente ha sido trazada por [15]. En primer lugar, se puede utilizar una versión modificada del sistema ampliamente aceptado de métricas de frecuencia recesiva-monetaria para definir el espacio de estados de los clientes o donantes. En segundo lugar, se describe un procedimiento para determinar la acción óptima de marketing directo en el espacio de acción discreta y continua para el individuo dado, basado en su posición en el espacio de estados. El procedimiento implica el uso de Q-learning libre de modelos para entrenar una red neural profunda que relaciona la posición de un cliente en el espacio de estado con las recompensas asociadas con las posibles actividades de marketing. La función de valor estimado sobre el espacio de estado de cliente puede interpretarse como valor de vida útil del cliente (CLV) y, por lo tanto, permite una estimación rápida de plug-in de CLV para un cliente dado. Se presentan los resultados experimentales, basados ​​en la Competición de Descubrimiento del Conocimiento y la Compilación de Herramientas de Minería de Datos, datos de correo de las solicitudes de donación.



\paragraph{Sistemas de Recomendaciones} La recomendación automática de música se ha convertido en un problema cada vez más relevante en los últimos años, ya que mucha música se vende y se consume digitalmente. La mayoría de los sistemas recomendados se basan en el filtrado colaborativo. En 2013, [16] propuso utilizar un modelo de factor latente para la recomendación, y predecir los factores latentes de audio de la música cuando no se puede obtener de los datos de uso. El enfoque tradicional se compara utilizando una representación de las palabras de las señales de audio con redes neuronales convolucionales profundas y las predicciones se evalúan cuantitativa y cualitativamente en el Million Song Dataset. El resultado demuestra que los recientes avances en DL traducen muy bien al ajuste de la recomendación musical, con profundas redes neuronales convolucionales que superan significativamente el enfoque tradicional. Los servicios en línea recientes dependen en gran medida de la personalización automática para recomendar contenido relevante a un gran número de usuarios. Esto requiere que los sistemas se escalen rápidamente para dar cabida a la corriente de nuevos usuarios que visitan los servicios en línea por primera vez. El trabajo de [17] en 2015 propuso un sistema de recomendación basado en el contenido para abordar tanto la calidad de la recomendación como la escalabilidad del sistema. También propusieron utilizar un rico conjunto de funciones para representar a los usuarios, de acuerdo con su historial de navegación web y consultas de búsqueda. Utilizan una aproximación DL para asignar usuarios y elementos a un espacio latente en el que se maximiza la similitud entre los usuarios y sus elementos preferidos. El análisis de escalabilidad muestra que el modelo DNN de varias vistas puede escalarse fácilmente para abarcar millones de usuarios y miles de millones de entradas de artículos.

\paragraph{Bioinformática} La anotación de la información genómica es un reto importante en biología y bioinformática. Las bases de datos existentes de funciones genéticas conocidas son incompletas y propensas a errores, y los experimentos bimoleculares necesarios para mejorar estas bases de datos son lentos y costosos. Aunque los métodos computacionales no son un sustituto de la verificación experimental, pueden ayudar de dos maneras: los algoritmos pueden ayudar en la curación de las anotaciones de genes al sugerir automáticamente imprecisiones, y pueden predecir funciones de genes previamente no identificadas, acelerando la tasa de descubrimiento de la función génica. En este trabajo [18], se desarrolla un algoritmo que logra ambos objetivos utilizando profundas redes neuronales de codificador automático. Con los experimentos sobre datos de anotación de genes del proyecto Gene Ontology, muestra que las redes profundas de codificadores automáticos logran un mejor rendimiento que otros métodos estándar de aprendizaje de máquinas, incluyendo la popular descomposición truncada de valores singulares.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% lista2
Las redes neuronales artificiales ({\em Artificial Neural Networks}, NN) han sido testigos de un renacimiento en el campo del ML con el surgimiento del DL \cite{Bengio2006, Hinton2006, Le2012, Ranzato2007}. Las principales ideas detrás del nuevo enfoque abarcan una gama de algoritmos \cite{Bengio2007, Hinton2006}, pero un principio unificador es que una NN con múltiples capas ocultas (que lo hacen profundo) puede codificar características cada vez más complejas en sus capas. Las NN fueron comunmente entrenadas a través del algoritmo de retropropagación \cite{Rumelhart1986b}, que utiliza el método del gradiente estocástico descendente ({\em Stochastics descent gradiente}, SGD), o una de sus variantes, para actualizar los pesos de la NN y de esa manera reducir el error total. No fue hasta el año 2006 que fue difundido que el método del gradiente disminuiría el valor del gradiente en las redes profundas. Sin embargo, los descubrimientos en los últimos años han demostrado que, con suficientes datos de entrenamiento y poder de procesamiento, el método de retropropagación y SGD resultan ser eficaces en la optimización de una NN masiva con millones de conexiones y muchas capas \cite{Ciresan2012, He2015, Le2012}. Esta realización ha llevado a registros sustantivos que se rompen en muchas áreas de las ML a través de la aplicación de la retropropagación en el aprendizaje profundo \cite{Ciresan2012, He2015, Le2012}, incluyendo el aprendizaje no supervisado \cite{Bengio2009}.




%Problemas que resuelve el Deep Learning (traducción automática, predicción de próxima palabra, identificación de imágenes en videos), las cosas top del Machine Learning hoy en día.

%Nos motiva el exito de las NN para predecir, nos motiva la dificultad del gradiente estocastico para minimizar el error en estos modelos no lineales complejos. Nos motiva la excelencia del SA en resolver problemas de optimización combinatoria que aparenemente pueden considerarse mas complejos que los problemas continuos.

%el gap: han tenido un desempeño relevante para resolver problemas de optimización combinatoria. Sorprende que no hayan sido explorados estos métodos

% Un párrafo para comparar los problemas conejillos de india con los que se compara. Para evaluar el desempeño de los algoritmos tipicamente se usan problemas problemas de aprendizaje de la literatura.

% sin embargo después recién en 2015 se retomar con ESTE PAPER, donde propone el uso de un método metaheurístico, esto sugiere que el SA puede tener un buen desempeño estudiando problemas de minimización, de este modo se plantea: SA tiene un desempeño computacional competitivo frente al GSD al resolver instancias de un problema de regresión tipica de la literatura (esto es la hipótesis nula)




%Aprendizaje profundo es hacer grandes avances en la solución de problemas que han resistido los mejores intentos de la comunidad de inteligencia artificial durante muchos años. Ha resultado ser muy bueno en el descubrimiento de estructuras complejas en datos de alta dimensión y por lo tanto es aplicable a muchos dominios de la ciencia, las empresas y el gobierno. Además de batir registros en el reconocimiento de imágenes [1-4] y el reconocimiento de voz [5-7], ha superado otras técnicas de aprendizaje de máquina en la predicción de la actividad de las moléculas de fármacos potenciales [8], el análisis de datos acelerador de partículas [9, 10] ], La reconstrucción de los circuitos cerebrales [11], y la predicción de los efectos de las mutaciones en el ADN no codificante en la expresión génica y la enfermedad [12, 13]. Tal vez más sorprendente, el aprendizaje profundo ha producido resultados muy prometedores para diversas tareas en la comprensión del lenguaje natural [14], en particular la clasificación de temas, análisis de sentimientos, respuesta a preguntas [15] y traducción de lenguajes [16,17].

La forma más común de aprendizaje automático, profundo o no, es el aprendizaje supervisado. Imagínese que queremos construir un sistema que pueda clasificar las imágenes como conteniendo, digamos, una casa, un automóvil, una persona o una mascota. Primero recopilamos un gran conjunto de datos de imágenes de casas, autos, personas y mascotas, cada uno etiquetado con su categoría. Durante el entrenamiento, la máquina muestra una imagen y produce una salida en forma de un vector de puntuaciones, uno para cada categoría. Queremos que la categoría deseada tenga la puntuación más alta de todas las categorías, pero es poco probable que suceda antes del entrenamiento. Se calcula una función objetivo que mide el error (o distancia) entre las puntuaciones de salida y el patrón de puntuaciones deseado. La máquina modifica entonces sus parámetros ajustables internos para reducir este error. Estos parámetros ajustables, a menudo llamados pesos, son números reales que pueden ser vistos como "perillas" que definen la función de entrada-salida de la máquina. En un típico sistema de aprendizaje profundo, puede haber cientos de millones de estos pesos ajustables y Cientos de millones de ejemplos etiquetados con los que entrenar la máquina.





%%%% TRADUCIDO POR GOOGLE
%El éxito de un SGD simple para lograr un rendimiento récord es quizás sorprendente. Después de todo, en un espacio de muchas dimensiones, el SGD debería ser susceptible al óptimo local, y a diferencia de un algoritmo evolutivo, todos sus huevos están esencialmente en una sola canasta porque funciona en efecto con una población de uno. Sin embargo, resulta empíricamente que SGD está penetrando más lejos hacia la optimalidad en redes de miles o millones de pesos que cualquier otro enfoque. Intentando en parte explicar este fenómeno, Dauphin et al. [11] hacen el intrigante argumento de que, de hecho, los espacios de peso ANN de muy alta dimensión proporcionan tantas rutas posibles desde cualquier punto dado que el óptimo local es realmente muy improbable. En cambio, sugieren que los verdaderos obstáculos para SGD son puntos de silla de montar, o áreas de largas y graduales mesetas de error. Esta visión ha ayudado a explicar por qué SGD no se atascan, y también a mejorar para moverse más rápido a lo largo de tales puntos de silla en el espacio de búsqueda. También hay variantes de SGD como RMSProp [48] que ayudan a sacarlo de situaciones similares.


%Si bien estos análisis pueden ayudar a explicar el éxito de SGD, también plantean una cuestión importante para la computación evolutiva (EC): Si de hecho hay tantos caminos hacia la optimalidad relativa en un espacio de peso ANN de alta dimensión, ¿por qué no Los mismos beneficios recibidos de este escenario por SGD también se aplican a EC? De hecho, quizás los algoritmos evolutivos (EAs) deberían incluso tener una ventaja. Después de todo, es posible que una población esté mejor adaptada que un solo individuo a una situación con muchas ramas prometedoras, y los EA sencillos son agnósticos respecto al índice de descenso con respecto a la pendiente del gradiente, lo que podría, en principio, Punto problema contemplado por Dauphin et al. [11]. En resumen, los argumentos de por qué SGD puede tener éxito en los espacios de dimensión extremadamente alta parecen a primera vista para apoyar o incluso favorecer a los EE.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% lista1
Las NN han sido ampliamente estudiadas y ampliamente utilizadas en muchas aplicaciones de la inteligencia artificial. El problema durante el proceso de aprendizaje de las NN es descrito como un problema de minimización de una función de error, la que depende de los pesos que conforman la red \cite{Rumelhart1986}. Este problema de optimización tiene la desventaja de ser no lineal, no convexo, además de tener mas de un mínimo local. Para solventar este problema se han desarrollado diversos algoritmos \cite{Grippo1994,Jacobs1988,Plagianakos2002,Rumelhart1986b,Plagianakos1998}  y su rendimiento varía según el problema a resolver.

El enfoque clásico para el entrenamiento de las NN es la aplicación de algoritmos basados en el gradiente como la retropropagación \cite{Rumelhart1986b}. El algoritmo de retropropagación busca minimizar la función de error mediante la dirección de descenso más pronunciada. Aunque la función de error disminuye rápidamente en la dirección del gradiente negativo, la retropropagación es generalmente ineficiente y poco fiable \cite{Gori1992} debido a la superficie de error. Además, su rendimiento se ve afectado por parámetros que deben ser especificados por el usuario, pues no existe una base teórica para escogerlos \cite{Nguyen1990}. Dichos parámetros tienen una importancia crucial en el buen funcionamiento del algoritmo, por lo que el diseñador está obligado a seleccionar parámetros como los pesos iniciales de la NN, la topología de la red y la tasa de aprendizaje. En diversas investigaciones \cite{Cauchy1847, Grippo1994, Plagianakos1998, Plagianakos2002} ha quedado demostrado que pequeñas modificaciones en estos valores influyen en el rendimiento de la NN.

Para proporcionar una convergencia más rápida y estable se han desarrollado diversas variaciones y alternativas a la retroprogación. Algunos de estos métodos son la adaptación de un término de momento \cite{Jacobs1988, Rumelhart1986b} o de una tasa variable de aprendizaje \cite{Jacobs1988,Vogl1988}. \citeA{Magoulas1997,  Plagianakos1998} propusieron dos técnicas para evaluar en forma dinámica la tasa de aprendizaje sin el uso de alguna heurística o alguna función adicional y las evaluaciones de gradiente. El primero se basó en el algoritmo de Barzilai y Borwein \cite{Barzilai1988} que adapta la tasa de aprendizaje sin evaluar la matriz Hessiana; mientras que el segundo utiliza estimaciones de la constante de Lipschitz, explotando la información local de la superficie de error y los pesos posteriores \cite{Magoulas1997}.% Hay evidencias \cite{Magoulas1997, Plagianakos2002, Plagianakos1998} que han demostrado que la retropropagación con algoritmos que adaptan la velocidad del aprendizaje son robustas y tienen un buen rendimiento para el entrenamiento de NN.

Se han sugerido métodos de segundo orden  para mejorar la eficiencia del proceso de minimización del error. Algunos de los métodos utilizados son el del gradiente conjugado \cite{Fletcher1964, Hestenes1952, Polak1969} y el quasi-Newton \cite{Huang1970, Nocedal2006}. Los métodos del gradiente conjugado utiliza una combinación lineal de la dirección de búsqueda anterior y el gradiente actual lo que produce una convergencia generalmente más rápida, es adecuado para redes neuronales de gran escala debido a su simplicidad, sus propiedades de convergencia y la poca memoria que requiere. En la literatura  se encuentran diversos métodos basados en el gradiente conjugado \cite{Birgin2001, Moller1993} que han sido utilizados para la construcción de NN en varias aplicaciones \cite{Charalambous1992, Peng2007, Sotiropoulos2002}. Los métodos quasi-Newton se consideran como los algoritmos más sofisticados para el entrenamiendo rápido de una NN. Definen la dirección de búsqueda mediante una aproximación de la matriz Hessiana, requiriendo información adicional. Se han propuesto soluciones para ajustar la aproximación Hessiana mediante la introducción de distintas estrategias \cite{Al-Baali1998, Nocedal1993, Oren1972, Oren1974, Yin2007}. Estas estrategias combinadas con búsquedas lineales no monótonas permitieron definir una convergencia superlineal y global \cite{Yin2007}, mejorando significativamente el rendimiento de los métodos originales.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

















\section{Descripción del problema}
% USAR [2003b] Squartini S, Hussain A, Piazza F
La retropropagación basa su funcionamiento en multiplicaciones sucesivas basadas en el error para poder calcular los gradientes, y a medida que el error se propaga hacia la capa de entrada de la red él gradiente comienza a disminuír su valor por cada capa que atraviesa. Esto significa que el gradiente disminuirá de manera exponencial, lo que representa un problema para redes profundas, ya que las capas mas cercanas a la capa de entrada necesitarán más tiempo para ser entrenadas.

El método de aprendizaje basado en simmulated annealing permite la actualización de los pesos de la red sin mermar la capacidad de adaptación de los pesos. El método supone una alternativa efectiva a los métodos tradicionales de aprendizaje para la convergencia de los métodos debido a la independencia que otorga a la actualización de los pesos de las distintas capas..


\section{Solución propuesta}
\subsection{Características de la solución}
Mediante el uso de el algoritmo {\em simulated annealing} se busca analizar la eficiencia que la NN alcanza en una red neuronal profunda frente a otros métodos de aprendizaje tales como SGD y RMSPROP.

\subsection{Propósito de la solución}
El propósito de la solución es aportar en el campo de las redes neuronales y la clasificación de datos, proporcionando un análisis comparativo de la convergencia de distintas redes.

\section{Objetivos y alcances del proyecto}
En ésta sección se presenta el objetivo general, los objetivos específicos además del alcance y limitaciones de la presenta investigación.

\subsection{Objetivo general}
Evaluar el desempeño del algoritmo {\em simulated annealing} y su efecto sobre el entrenamiento de redes neuronales profundas en comparación con otros métodos.

\subsection{Objetivos específicos}
Los objetivos establecidos para el presente trabajo son descritos a continuación
\begin{enumerate}
	\item Definir las reglas de aprendizaje a comparar.
	\item Construir los conjuntos de datos de entrada y salida a analizar.
	\item Establecer los parámetros de las redes neuronales para la experimentación.
	\item Establecer los algoritmos de aprendizaje a comparar.
	\item Entrenar las redes con los distintos conjuntos de datos.
	\item Establecer las conclusiones del trabajo.
\end{enumerate}

\subsection{Alcances}
\begin{enumerate}
	\item Se analizará la misma arquitectura con diferentes reglas de aprendizaje.
	\item Los conjunto de datos para el entrenamiento a utilizar son los propuestos en \cite{Morse2016}.
\end{enumerate}

\section{Metodología y herramientas utilizadas}
\subsection{Metodología de trabajo}
Considerando el aspecto investigativo del trabajo, se considera la utilización del método científico. Entre las actividades que componen la metodología, \citeA{Sampieri2006} describe los siguientes pasos para desarrollar una investigación:

\begin{itemize}
	\item Formulación de la hipótesis: Las redes neuronales que adolecen del desvanecimiento del gradiente se ven beneficiadas por el uso del algoritmo {\em simulated annealing} en la convergencia.

	\item Marco teórico: Una revisión de la literatura donde se aborda el problema planteado, para situarse en el contexto actual de los problemas. Se describirán redes neuronales que buscan solucionar el mismo problema.

	\item Diseño de la solución: Se deberá diseñar el experimento para generar los datos que permitan sustentar las comparaciones entre las distintas redes.% Diseñar y ejecutar el experimento basado en entradas equivalentes.

	\item Análisis y verificación de los resultados: Los resultados se analizarán considerando los valores de convergencia de los distintos métodos.

	\item Presentación de los resultados: Se presentarán tablas que describan los resultados obtenidos y que se consideren pertinentes.

	\item Conclusiones obtenidas en el desarrollo de la investigación.
\end{itemize}

\subsection{Herramientas de desarrollo}
Para el desarrollo y ejecución de los experimentos se utilizará un equipo con las siguientes características
\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|}\hline
		Sistema Operativo	& Solus 2017.04.18.0 64-bit\\\hline
		Procesador				 & Intel$^\circledR$ Core\texttrademark i5-2450M CPU @ 2.50GHz x 4\\\hline
		RAM							  & 7.7Gb\\\hline
		Gráficos					& Intel$^\circledR$ Sandybridge Mobile\\\hline
		Almacenamiento	   & 935.6 GB\\\hline
	\end{tabular}
	\caption{Especificaciones del equipo}
\end{table}

El software que se utilizará es:
\begin{itemize}
	%\item Plataforma de desarrollo: Atom.
	\item Lenguaje de programación: Python.
	\item Sistema de redes neuronales: Keras API \cite{Keras2015}.
	\item Herramienta ofimática: \LaTeX.
\end{itemize}
