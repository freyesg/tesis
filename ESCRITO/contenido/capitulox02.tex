\chapter{Aspectos teóricos y revisión de la literatura}

En esta sección se abarcan los aspectos relacionados al conocimiento general para la comprensión del presente trabajo (aspectos teóricos) y la revisión de la literatura asociada al trabajo presentado en esta tesis. %Para realizar un análisis de la Programación Genética aplicada a problemas NP-Hard es necesario conocer la base teórica de ésta. Para ello, se exponen los conceptos fundamentales de la computación evolutiva y la Programación Genética. La sección 2.1 se centra en explicar aquellas partes fundamentales al tema que se trata en esta tesis. De la revisión de la literatura se desprende lo presentado en la sección 2.2.

%%%%%%%%%%%%% IDEA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Un párrafo para comparar los problemas conejillos de india con los que se compara. Para evaluar el desempeño de los algoritmos tipicamente se usan problemas problemas de aprendizaje de la literatura.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%% IDEA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Las características del SGD y el problema del desvanecimiento. Las heurísticas y metaheurísticas.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Aspectos teóricos}
\subsection{Redes neuronales multicapas}
Una de las características que diferencia a las neuronas biológicas del resto de las células vivas, es su capacidad de comunicación. En la figura \ref{fig:neurona} se puede apreciar un esquema general de una neurona biológica. Las dendritas y el soma (cuerpo celular) reciben las señales de entrada; el cuerpo celular las combina e integra y emite una señal de salida. El axón transporta esas señales a los terminales axónicos, que se encargan de distribuir información a un nuevo conjunto de neuronas. Por lo general, una neurona recibe información de miles de otras neuronas, y a su vez, envía información a otras neuronas, formando una red de conexiones.
\begin{imagen}
	\scalebox{0.07}{\input{img/neurona}}
	\caption{Esquema de una neurona biológica.}
	\label{fig:neurona}
\end{imagen}

Una NN es un sistema de procesamiento basado en las conexiones que componen las redes neuronales biológicas y en la forma en que estas se comunican. Cada neurona tiene un estado interno, llamado de {\em activación}, que es una función de las entradas recibidas y envía su activación como señal a varias otras neuronas. Cada una está conectada a otras neuronas por medio de enlaces de comunicación dirigidos hacia adelante, formando capas, donde cada conexión tiene un peso asociado como se muestra en la figura \ref{fig:nn}. Los pesos representan la información que está siendo utilizada por la red para resolver un problema.
\begin{imagen}
	\scalebox{0.8}{\input{img/neural_network_img}}
	\caption{Esquema de una red neuronal}
	\label{fig:nn}
\end{imagen}

La activación de una neurona $Y$ está dado por su función de activación y las entradas. Alguna de las funciones de activación mas comunes se pueden ver en la tabla \ref{tab:f_activacion}. La entrada a la neurona $Y$ corresponde a la suma ponderada de los pesos de las conexiones que llegan hacia $Y$ por la salida de las neuronas de la capa anterior. En la figura \ref{fig:nn} se puede ver un modelo de tres capas, con una capa oculta. donde las salidas de las neuronas de la capa de entrada llegan hacia las neuronas $Y_1$ e $Y_2$ de la capa oculta para aplicar su función de activación respectiva, y sus salida son enviadas a la capa de salida.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|}\hline
		{\bf Función}	& {\bf Fórmula}	& {\bf Rango}\\\hline
		Identidad & $f(x) = x$	& $[-\infty, \infty]$\\\hline
		Lineal por tramos &
		$f(x) = \left\{
		\begin{array}{ll}
			-1		& x < -1\\
			a*x		& -1 \leq x \leq 1\\
			1		& x > 1
		\end{array}
		\right. $	& $[-1, 1]$\\\hline
		Sinusoidal	& $ f(x) = \sin(\omega x + \varphi) $	& $[-1, 1]$\\\hline
		Sigmoidal	& $f(x) = \frac{1}{1 + \exp{-x}}$	& $[0, 1]$\\\hline
		Tangente hiperbólica	& $\frac{1 - \exp(-x)}{1 + \exp(-x)}$	& $[-1, 1]$\\\hline
	\end{tabular}
	\caption{Algunas funciones de activaciones.}
	\label{tab:f_activacion}
\end{table}

Las NN multicapa definen una relación entre la entrada y la salida. Esta relación se obtiene propagando hacia adelante los valores de las variables de entrada, es por esto que también se les llama redes {\em feedforward}. Cada neurona de la red procesa la entrada recibida y produce una respuesta que se propaga, mediante las conexiones, hacia las neuronas de la capa siguiente.

Existen dos fases importante dentro del modelo
\begin{itemize}
	\item Fase de entrenamiento: Se usa un conjunto de datos o patrones de entrenamiento para determinar los pesos que definen el modelo de la NN. Se calculan de manera iterativa, de acuerdo con los valores de entrenamiento, con el objeto de minimizar el error cometido entre la salida obtenida por la NN y la salida deseada.

	Los pesos óptimos se obtienen minimizando una función. Uno de los criterios utilizados es la minimización del error cuadrático medio entre el valor de salida y el valor real esperado.

	\item Fase de prueba: Durante el entrenamiento, el modelo se ajusta al conjunto de entrenamiento, perdiendo la habilidad de generalizar su aprendizaje a casos nuevos, a esta situación se le llama sobreajuste.

	Para evitar el sobreajuste, se utiliza un segundo grupo de datos diferentes, el conjunto de validación, que permitirá controlar el proceso de aprendizaje.
\end{itemize}


\subsection{El algoritmo de entrenamiento por retropropagación y el desvanecimiento del gradiente}
El algoritmo de retropropagación del error, también conocido como la regla delta, fue el primer algoritmo de entrenamiento para redes multicapas  \cite{Werbos1974, Rumelhart1986}. El término retropropagación es utilizado debido a la forma de implementar el método del gradiente en las redes multicapa, pues el error cometido en la salida de la red es propagado hacia atrás, transformándolo en un error para cada una de las neuronas ocultas de la red. El entrenamiento de una red por retropropagación implica tres etapas: la propagación del patrón de entrada, el cálculo del error y su propagación hacia las capas anteriores, y el ajuste de los pesos. Después del entrenamiento, la aplicación de la red implica solamente los cálculos de la fase de propagación. En caso de que el entrenamiento sea lento, una red ya entrenada puede producir su salida rápidamente.

El funcionamiento del agoritmo de retropropagación se puede apreciar en la figura \ref{fig:backprop}. Se representa en azul primera fase del algoritmo, donde la entrada de la red se propaga hacia la salida a través de las neuronas transformandola en cada neurona de la red. La segunda fase, en verde, muestra como se propaga, desde la salida, el error hacia las capas anteriores. Finalmente, se puede ver en rojo como el algoritmo actualiza los pesos de la red utilizando el error que genera la red.
\begin{imagen}
	\scalebox{1.0}{\input{img/nn_parada}}
	\caption{Esquema del algoritmo de retropropagación.}
	\label{fig:backprop}
\end{imagen}

Si un perceptrón multicapa con $C$ capas y $n_c$ neuronas en la capa $c$, donde $W_c = (w^{c}_{ij})$ es la matriz de pesos, $w^{c}_{ij}$ representará el peso de la conexion de la neurona $i$ de la capa $c$ hasta la neurona $j$ de la capa siguiente. Denotaremos $a^{c}_{i}$ a la activación de la neurona $i$ de la capa $c$ que se calcula de la siguiente manera:
\begin{itemize}
	\item {\bf Activación de una neurona de la capa de entrada}: Las neuronas se encargan de transmitir la entrada recibida, por lo tanto $$ a^{1}_{i} = x_{i}, i = 1, 2, \cdots, n$$ donde $X = (x_1, x_2, \cdots, x_n)$ representa el vector de entrada.

	\item {\bf Activación de una neurona de la capa oculta}: Las neuronas de una capa oculta procesa la información recibida aplicando la función $f$ a la suma de los productos de la entrada por sus pesos, es decir $$ a^{c}_{i} = f\left(\sum^{n_{c - 1}}_{j=1} w^{c - 1}_{ji}a^{c - 1}_{j} + \theta^{c}_{i}\right), i = 1, 2, \cdots, n_c; c = 2, 3, \cdots, C - 1$$ donde $a^{c - 1}_{j}$ es la salida de la capa anterior a $c$.

	\item {\bf Activación de una neurona de la capa de salida}: La activación de una neurona de la capa de salida viene dada por la función $f$ aplicada a la suma de los productos de la entrada por sus pesos, es decir $$ y_{i} = a^{c}_{i} = f\left(\sum^{n_{c - 1}}_{j=1} w^{C - 1}_{ji}a^{C - 1}_{j} + \theta^{C}_{i}\right), i = 1, \cdots, n_c$$ donde $Y = (y_1, y_2, \cdots, y_{n_{c}})$ es el vector de salida.
\end{itemize}

La función $f$ es la función de activación de la neurona. Aunque existe gran variedad de funciones de activación (ver tabla \ref{tab:f_activacion}), las funciones de activación mas utilizadas son la sigmoidal y la tangente hiperbólica, descritas en las escuaciones \ref{eq:sigm} y \ref{eq:tanh} respectivamente.
\begin{eqnarray}
	f_{sigm}(x) &=& \frac{1}{1+\exp(-x)}\label{eq:sigm}\\
	f_{tanh}(x) &=& \frac{1 - \exp(-x)}{1 + \exp(-x)}\label{eq:tanh}
\end{eqnarray}

Ambas funciones poseen como imagen un intervalo de valores entre $[0, 1]$ y $[-1, 1]$ como se observa en la figura \ref{fig:funciones}.% y están descritas por las ecuaciones \ref{eq:sigm} y \ref{eq:tanh}.

\begin{imagen}
	\scalebox{1.0}{\input{img/funciones}}
	\caption{Funciones de activación mas utilizadas.}
	\label{fig:funciones}
\end{imagen}

Las NN multicapa actualizan sus pesos en función de una regla de aprendizaje, de tal manera que los nuevos pesos permitan reducir el error de salida. Por tanto, para cada patrón de entrada necesario disponer de una de salida deseada. El objetivo es que la salida de la red sea lo más próxima posible a la salida deseada, debido a esto es que el aprendizaje de la red se describe como un problema de minimización de la siguiente manera $$ \min_{W} E $$ donde $W$ es el conjunto de parámetros de la red (pesos y umbrales) y $E$ es una función de error que evalúa la diferencia entre las salidas de la red y las salidas deseadas. En la mayor parte de los casos, la función de error se define como:
\begin{eqnarray}
	E = \frac{1}{N}\sum^{N}_{i = 1} e(i)
\end{eqnarray}

Donde $N$ es el número de muestras y $e(n)$ es el error cometido por la red para el patrón $i$, definido de la siguiente manera
\begin{eqnarray}
	e(i) = \frac{1}{n_{C}}\sum^{n_{C}}_{j = 1} (s_{j}(i) - y^{j}(n))^2\label{eq:error_patron}
\end{eqnarray}

Siendo $Y(i) = (y_{1}(i), y_{2}(i), \cdots, y_{n_{C}}(i))$ y $S(i) = (s_{1}(i), s_{2}(i), \cdots, s_{n_{C}}(i))$ los vectores de salida y salidas deseadas para el patrón $i$ respectivamente.

De esta manera, si $W^{*}$ es un mínimo de la función de error $E$, en dicho punto el error será cercano a cero, y en consecuencia, la salida de la red será próxima a la salida deseada. La presencia de funciones de activación no lineales hace que la respuesta de la red sea no lineal respecto a los parámetros ajustables, por lo que el problema de minimización es un problema no lineal y se hace necesario el uso de técnicas de optimización no lineales para su resolución.

Las técnicas de actualización utilizadas suelen basarse en la actualización de los parámetros de la red mediante la determinación de una dirección de búsqueda. En el caso de las NN multicapa, la dirección de búsqueda más utilizada se basa en la dirección contraria del gradiente de la función de error $E$, el método de gradiente descendente. El procedimiento está basado en una sucesión de minimizaciones del error $e(i)$ por cada patrón, en lugar de minimizar el error total $E$ de la red. Aplicando el método cada parámetro $w$ se modifica según la siguiente regla de aprendizaje
\begin{eqnarray}
	w(i) = w(i - 1) - \alpha\frac{\partial e(i)}{\partial w}\label{eq:update}
\end{eqnarray}
donde $e(i)$ es el error para el patrón de entrada $i$ dado por la ecuación \ref{eq:error_patron}, y $\alpha$ es la tasa de aprendizaje, éste último determina el desplazamiento en la superficie del error.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DESVANECIMIENTO DEL GRADIENTE
A medida que el error se propaga a través de la red, los pesos se actualizarán según la ecuación \ref{eq:update}, utilizando funciones cuyo gradiente se encuentra entre 0 y 1. Debido a que estos gradientes se multiplican durante la retropropagación, tienden a {\em desvanecerse} a través de las capas, y en una NN profunda esto evita que los pesos de las primeras capas ocultas se actualicen minimamente.

Si se tiene una NN, la activación de una neurona de una capa intermedia $i$ con función de activación $f$ es $y^{i}(t) =  f_{i}(net_{i}(t))$ donde $$ net_{i}(t) = \sum_{j}w_{ji}y^{j}(t - 1) $$ es la entrada a la neurona. Además $w_{ji}$ es el peso de la conexión desde la unidad $j$ de la capa anterior hasta la unidad $i$ de la capa actual, $d_{k}(t)$ será la respuesta esperada de la unidad $k$ de la capa de salida en el tiempo $t$. Usando el error cuadrático medio ({\em Mean square error}, MSE), el error de $k$ será
$$ E_{k}(t) = (d_{k}(t) - y^{k}(t))^2 $$



%Si se tiene una NN, la activación de una neurona de una capa intermedia $i$ con función de activación $f_i$ y con entrada $$ net_{i}(t) = \sum_{j}w_{ji}y^{j}(t - 1) $$ es $$y^{i}(t) = f_{i}(net_{i}(t))$$ Además $w_{ji}$ es el peso de la conexión desde la unidad $j$ de la capa anterior hasta la unidad $i$ de la capa actual, $d_{k}(t)$ será la respuesta esperada de la unidad $k$ de la capa de salida en el tiempo $t$. Usando el error cuadrático medio ({\em Mean square error}, MSE), el error de $k$ será $$ E_{k}(t) = (d_{k}(t) - y^{k}(t))^2 $$

En un tiempo $\tau \leq t$ cualquiera, el error de una neurona $j$ que no sea una neurona de entrada es la suma de los errores externos y el error propagado hacia atrás desde la neurona previa será
$$ \vartheta_{j}(\tau) = f'_{j}(net_{j}(\tau))\left(E_{j}(\tau) + \sum_{i} w_{ij}\vartheta_{i}(\tau + 1)\right) $$

El peso actualizado en el tiempo $\tau$ resulta $w_{jl}^{new} = w_{jl}^{old} + \alpha\vartheta_{j}(\tau) y^{l}(\tau - 1)$ donde $\alpha$ es la tasa de aprendizaje, y $l$ es una unidad arbitraria conectada a la unidad $j$.

%\citeA{Puskorius1994}
La propagación hacia atrás de un error que ocurre en una unidad $u$ en un tiempo $t$ hacia una unidad $v$ para $q$ pasos, escala el error de la siguiente manera
\begin{eqnarray}
\frac{\partial\vartheta_{v}(t - q)}{\partial\vartheta_{u}(t)} =
\left\{
\begin{array}{lr}
	f^{'}_{v}(net_{v}(t - 1))w_{uv}	& q = 1\\
	\\
	f^{'}_{v}(net_{v}(t - q))\sum^{n}_{l=1}\frac{\partial\vartheta(t - q + 1)}{\partial\vartheta_{u}(t)}w_{lv}	& q > 1
\end{array}
\right.
\end{eqnarray}

Con $l_{q} = v$ y $l_{0} = u$, el factor de escalamiento es
\begin{eqnarray}
\frac{\partial\vartheta_{v}(t - q)}{\partial\vartheta_{u}(t)} =
\sum^{n}_{l_{1}=1}\cdots\sum^{n}_{l_{q - 1}=1}\prod^{q}_{m = 1}f^{'}_{l_{m}}(net_{l_{m}}(t - m))w_{l_{m}l_{m - 1}}\label{eq:vanishing}
\end{eqnarray}

La sumatoria de los $n^{q - 1}$ términos $\prod^{q}_{m = 1}f^{'}_{l_{m}}(net_{l_{m}}(t - m))w_{l_{m}l_{m - 1}}$ escalan el error. Los distintos términos pueden tener signos diferentes, por lo tanto, el aumento del número de unidades $n$ no implica un incremento del error absoluto. Pero con mas unidades se incrementa la expectativa de que el valor absoluto del error aumente. Si $\rho(m, l_{m}, l_{m - 1}) := |f^{'}_{l_{m}}(net_{l_{m}}(t - m))w_{l_{m}l_{m - 1}}| < 1.0$
para todo $m$, el producto en (\ref{eq:vanishing}) decrece exponencialmente con $q$, es decir, el error se desvanece como muestra la figura \ref{fig:vanishing}. Un error que se desvanece a lo largo del flujo casi no tiene efecto en la actualización de los pesos. %Dada la constante $y^{l_{m - 1}} \neq 0$, $\rho(m, l_{m}, l_{m - 1})$ es máximo cuanto $w_{l_{m}l_{m - 1}} = \frac{1}{y^{l_{m - 1}}}\coth(\frac{net_{l_{m}}}{2})$.

\begin{imagen}
	\scalebox{1.0}{\input{img/vanishing_img}}
	\caption{Gradiente descendente}
	\label{fig:vanishing}
\end{imagen}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Revisión de la literatura}
\subsection{El gradiente estocástico descendente}
% https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16
% http://leon.bottou.org/publications/pdf/nimes-1991.pdf
% http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent
% Amari1993
El gradiente estocástico descendente es una drástica simplificación del método tradicional. En vez de calcular el gradiente exacto de la función de error $E$, en cada iteración se estima el valor del gradiente basado en un ejemplo escogido de manera aleatoria. El algoritmo de descenso de gradiente tradicional actualiza los parámetros $W$ según la siguiente expresión
$$ w^{(i)} = w^{(i - 1)} - \alpha\frac{\partial E^{(i)}}{\partial w} $$ y en el algoritmo \ref{alg:gd} se describe su funcionamiento en términos generales.

\begin{algorithm}[H]
 \For{uno o más epocas}{
	 \For{cada peso $j$}{
		 $w_{j} := w + \Delta w_{j}$ donde $\Delta w_{j} = \alpha\sum_{i}(target^{i} - output^{i})x_{j}^{i}$
	 }
 }
 \caption{Algoritmo del gradiente descendente}
 \label{alg:gd}
\end{algorithm}

Donde el resultado obtenido es aproximado evaluando el costo y el gradiente sobre el conjunto de entrenamiento completo. El gradiente estocástico descendente (SGD) elimina la necesidad de analizar el conjunto de entrenamiento en cada iteración, calculando el gradiente de los parámetros utilizando sólo un único, o pocos, ejemplos de entrenamiento como muestra el algoritmo \ref{alg:sgd}. La forma de actualización que propone el método SGD está dada por la ecuación \ref{eq:sgd}.
\begin{eqnarray}
	w^{(i)} = w^{(i - 1)} - \alpha\frac{\partial E(x^{(i)}, y^{(i)})}{\partial w}\label{eq:sgd}
\end{eqnarray}

Con un par $(x^{(i)}, y^{(i)})$ del conjunto de entrenamiento.

Generalmente cada actualización de parámetros en SGD analiza algunos ejemplos de entrenamiento en lugar de un solo ejemplo. Esto reduce la varianza en la actualización de parámetros y puede conducir a una convergencia más estable. Un tamaño de minibatch típico es 256, aunque el tamaño óptimo del minibatch puede variar para diferentes aplicaciones y arquitecturas.

Un punto final pero importante con respecto a SGD es el orden en que presentamos los datos al algoritmo. Si los datos se dan en algún orden significativo, esto puede sesgar el gradiente y conducir a la convergencia pobres. Generalmente un buen método para evitar esto es mezclar aleatoriamente los datos antes de cada época de entrenamiento.

\begin{algorithm}[H]
	\For{uno o más epocas o el costo mínimo deseado ha sido alcanzado}{
		\For{ejemplo de entrenamiento $i$ aleatorio}{
			\For{cada peso $j$}{
				$w_{j} = w + \alpha(target^{i} - output^{i})x_{j}^{i}$\;
			}
		}
	}
	\caption{Algoritmo del gradiente descendente estocástico}
	\label{alg:sgd}
\end{algorithm}





\subsection{Retropropagación resiliente y RMSProp}
En las capas ocultas de las NN se utilizan con frecuencia funciones de activación sigmoidales, y estas reducen la entrada a un rango finito de salida. Se caracterizan por sus pendientes próximas a cero para entradas muy grandes, y esto representa un problema cuando se utiliza el gradiente descendente para el entrenamiento, pues se acentúa el problema del desvanecimiento del gradiente.

\citeA{Riedmiller1993} presenta el algoritmo de retropropagación resiliente ({\em Resilient retropropagation}, RPROP), con el que busca eliminar los efectos del desvanecimiento del gradiente, esto mediante el uso del signo de la derivada para determinar la dirección del ajuste de los pesos, haciendo que la magnitud de la derivada no tenga efecto sobre la actualización de los pesos de la NN. Los pesos de la NN se incrementarán por un factor $\Delta{i}$ cuando la derivada de la función respecto a dicho peso tenga el mismo signo que las dos iteraciones anteriores. Mientras que si la derivada de la función respecto a dicho peso cambia de signo respecto de la iteración anterior los pesos se decrementarán por un factor $\Delta_{d}$. En caso de que la derivada de la función respecto de dicho peso sea igual a cero, el valor de actualización no varía.

%\subsubsection{Root Mean Square RPROP}
En 2012 \citeA{Tieleman2012} presentan una variante del algoritmo RPROP, el algoritmo de la raíz cuadratica media de retropropagación resiliente ({\em Root mean square resilient retropropagation}, RMSPROP). Se propone mantener una media móvil del cuadrado del gradiente para cada peso
\begin{eqnarray}
	MeanSquare(w, t) &=& 0.9MeanSquare(w, t - 1) + 0.1\frac{\partial E}{\partial w^{(t)}}^{2}
\end{eqnarray}
Y dividiendo el gradiente por $\sqrt{MeanSquare(w, t)}$ hace que el aprendizaje funcione de mejor manera.


\subsection{Heurísticas y meta-heurísticas}
Hoy en día, resolver problemas computacionalmente complejos precisa el desarrollo de algoritmos más avanzados. Los algoritmos exactos a menudo utilizan una gran cantidad de tiempo debido al tamaño del espacio de soluciones factibles. Para resolver este inconveniente, se han diseñado algoritmos aproximados basados en el uso de heurísticas y meta-heurísticas, los que permiten encontrar soluciones que se aproximan a la mejor solución. Estos algoritmos utilizan funciones que están diseñadas para encontrar el espacio de soluciones de forma inteligente.

La figura \ref{fig:tax_opt} muestra la clasificación de diferentes problemas de optimización, los que se categorizan en: algoritmos exactos y algoritmos aproximados \cite{Desale2015}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{img/tax_opt.png}
    \caption{Métodos para resolver problemas de optimización \protect\cite{Desale2015}.}
		\label{fig:tax_opt}
\end{figure}
Los algoritmos meta-heurísticos son un proceso iterativo que guía una heurística para explorar el espacio de búsqueda. Estos tipos de algoritmos son utilizados para encontrar una solución en un espacio de búsqueda discreto en problemas de optimización combinatoria.

\subsubsection{Simulated Annealing}
%Desde Kirkpatrick et al., [23] y Cerny [11] introdujeron los conceptos de recocido [30] en el campo de la optimización combinatoria, se ha prestado mucha atención a la investigación sobre la teoría y las aplicaciones de SA [25]
% https://link.springer.com/chapter/10.1007/978-3-642-83069-3_15
% lista8
El método {\em simulated annealing} (SA) es un algoritmo de búsqueda meta-heurística para problemas de optimización combinatoria introducido por \cite{Kirkpatrick1983} y \cite{Cerny1985}. La técnica está basada en el proceso de calentamiento de un metal, vidrio o cristal hasta su punto de fusión, para luego enfriarlo hasta obtener una estructura cristalina. El algoritmo \ref{alg:sa} decribe el procedimiento de la siguiente manera: dada una solución inicial se genera un conjunto de soluciones que son una perturbación de la solución actual, en el caso de que al evaluar la perturbación se obtenga un mejor desempleo que la solución actual, se actualizará la solución actual; y en caso contrario, existirá una probabilidad de que la solución actual sea actualizada de igual manera.

\scalebox{0.8}{\begin{algorithm}[H]
    \SetAlgoLined
	\DontPrintSemicolon
	\KwData{Temperatura $T$, constante de Boltzmann $k$, factor de reducción $c$}
	Seleccionar un vector solución $x_{0}$\;
	\While{El criterio no se cumpla}{
		\While{Existan soluciones en el conjunto}{
			Seleccionar una solución $x_{0} + \Delta x$\;
			\uIf{$f(x_{0} + \Delta x) < f(x_{0})$}{
				$f_{new} = f(x_{0} + \Delta x)$; $x_{0} = x_{0} + \Delta x$\;
			}
			\Else{
				$\Delta f = f(x_{0} + \Delta x) - f(x_{0})$\;
				\uIf{$rand(0, 1) > \exp{(-\Delta f/kT}$)}{
					$f_{new} = f(x_{0} + \Delta x)$; $x_{0} = x_{0} + \Delta x$\;
				}\Else{
					$f_{new} = f(x_{0})$\;
				}
			}
			$f = f_{new}$; $T = cT$\;
		}
	}
	\caption{Simulated annealing}
	\label{alg:sa}
\end{algorithm}}



%\subsection{Instancias}
%El desarrollo de algoritmos cada vez más sofisticados precisa de un parámetro común de comparación. Estos algoritmos incluyen distintas técnicas, los que han de someterce a estudios experimentales sobre conjuntos de datos que permiten analizar el rendimiento que poseen, donde estos conjuntos suelen ser públicos.

%%El teorema de \textit{no free lunch} (NFL) indica que no existe un algoritmo supere a los existentes en todas las instancas disponibles de un problema. Si un algoritmo se declarase como mejor que otro conjunto de algoritmos, entonces es posible esperar que existan instancias que aún no han sido probadas. Es por esto que una de las claves para poder realizar un correcto estudio es caracterizar las instancias del problema de acuerdo a su dificultad y al espacio de soluciones que las representa \citep{smith_2012}.

%%Las características más sencillas de una instancia para un problema de optimización son las que se definen como parte de la sub-clase de la instancia: características como el número de variables y restricciones, o si las matrices que almacenan los parámetros son simétricas, etc.

%%Para el PCMR, una forma de clasificar las instancias es mediante la distribución de costos que posee (distancias entre los nodos o ciudades), donde se muestra el número de valores distintos que poseen las distancias \citep{smith_2012}. Las instancias a utilizar para este problema, son obtenidas de la TSPLib \citep{reinelt_1991}. Esta librería contiene instancias ampliamente utilizadas en la literatura, las que en su totalidad cuentan con su valor óptimo y, en algunos casos, con una posible solución.
