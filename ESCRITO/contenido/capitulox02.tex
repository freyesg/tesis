\chapter{Aspectos teóricos y revisión de la literatura}

En esta sección se abarcan los aspectos relacionados al conocimiento general para la comprensión del presente trabajo (aspectos teóricos) y la revisión de la literatura asociada al trabajo presentado en esta tesis. %Para realizar un análisis de la Programación Genética aplicada a problemas NP-Hard es necesario conocer la base teórica de ésta. Para ello, se exponen los conceptos fundamentales de la computación evolutiva y la Programación Genética. La sección 2.1 se centra en explicar aquellas partes fundamentales al tema que se trata en esta tesis. De la revisión de la literatura se desprende lo presentado en la sección 2.2.

%%%%%%%%%%%%% IDEA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Un párrafo para comparar los problemas conejillos de india con los que se compara. Para evaluar el desempeño de los algoritmos tipicamente se usan problemas problemas de aprendizaje de la literatura.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%% IDEA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Las características del SGD y el problema del desvanecimiento. Las heurísticas y metaheurísticas.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Aspectos teóricos}
\subsection{Redes neuronales multicapas}
Una de las características que diferencia a las neuronas biológicas del resto de las células vivas, es su capacidad de comunicación. En la figura \ref{fig:neurona} se puede apreciar un esquema general de una neurona biológica. Las dendritas y el soma (cuerpo celular) reciben las señales de entrada; el cuerpo celular las combina e integra y emite una señal de salida. El axón transporta esas señales a los terminales axónicos, que se encargan de distribuir información a un nuevo conjunto de neuronas. Por lo general, una neurona recibe información de miles de otras neuronas, y a su vez, envía información a otras neuronas, formando una red de conexiones.
\begin{imagen}
	\scalebox{0.07}{\input{img/neurona}}
	\caption{Esquema de una neurona biológica.}
	\label{fig:neurona}
\end{imagen}


Una NN es un sistema de procesamiento basado en las redes neuronales biológicas y en la forma en que estas se conectan y comunican. Cada neurona tiene un estado interno, llamado de {\em activación}, que es una función de las entradas recibidas y envía su activación como señal a varias otras neuronas. Cada una está conectada a otras neuronas por medio de enlaces de comunicación dirigidos, formando capas, donde cada conexión tiene un peso asociado como se muestra en la figura \ref{fig:nn}. Los pesos representan la información que está siendo utilizada por la red para resolver un problema.
\begin{imagen}
	\scalebox{0.8}{\input{img/neural_network_img}}
	\caption{Esquema de una red neuronal}
	\label{fig:nn}
\end{imagen}

La activación de una neurona $Y$ está dado por su función de activación y las entradas. Alguna de las funciones de activación mas comunes se pueden ver en la tabla \ref{tab:f_activacion}. La entrada a la neurona $Y$ corresponde a la suma ponderada de los pesos de las conexiones que llegan hacia $Y$ por la salida de las neuronas de la capa anterior. En la figura \ref{fig:nn} se puede ver como las salidas de las neuronas de la capa de entrada llegan hacia las neuronas $Y_1$ e $Y_2$ para aplicar su función de activación respectiva, y sus salida son enviadas a la capa de salida.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|}\hline
		{\bf Función}	& {\bf Fórmula}	& {\bf Rango}\\\hline
		Identidad & $f(x) = x$	& $[-\infty, \infty]$\\\hline
		Lineal por tramos &
		$f(x) = \left\{
		\begin{array}{ll}
			-1		& x < -1\\
			a*x		& -1 \leq x \leq 1\\
			1		& x > 1
		\end{array}
		\right. $	& $[-1, 1]$\\\hline
		Sinusoidal	& $ f(x) = \sin(\omega x + \varphi) $	& $[-1, 1]$\\\hline
		Sigmoidal	& $f(x) = \frac{1}{1 + \exp{-x}}$	& $[0, 1]$\\\hline
		Tangente hiperbólica	& $\frac{1 - \exp(-x)}{1 + \exp(-x)}$	& $[-1, 1]$\\\hline
	\end{tabular}
	\caption{Algunas funciones de activaciones.}
	\label{tab:f_activacion}
\end{table}

\subsection{El algoritmo de retropropagación}
El algoritmo de retropropagación del error, también conocido como la regla delta, fue el primer algoritmo de entrenamiento para redes multicapas  \cite{Werbos1974, Rumelhart1986}. El entrenamiento de una red por retropropagación implica tres etapas: la propagación del patrón de entrada, el cálculo del error y su propagación hacia las capas anteriores, y el ajuste de los pesos. Después del entrenamiento, la aplicación de la red implica solamente los cálculos de la fase de propagación. En caso de que el entrenamiento sea lento, una red ya entrenada puede producir su salida rápidamente.

Como se puede apreciar en la figura \ref{fig:backprop}, la entrada de la red se propaga hacia la salida a través de las neuronas transformandola en cada neurona de la red.
\begin{imagen}
	\scalebox{1.0}{\input{img/nn_parada}}
	\caption[Algoritmo de retropropagación]{Algoritmo de retropropagación. En azul se puede apreciar la forma en que la señal se mueve a través de la red hasta generar una salida, mientras que en rojo se muestra como la red propaga el error a través de la red mientras actualiza los pesos.}
	\label{fig:backprop}
\end{imagen}

\section{Revisión de la literatura}
\subsection{El gradiente estocástico descendente}
\subsection{Prop y RMSProp}
\subsection{Heurísticas y metaheurísticas}
\subsection{Simulated Annealing}
\subsection{Instancias}
