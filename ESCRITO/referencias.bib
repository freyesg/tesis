@article{Al-Baali1998,
	author="Al-Baali, M.",
	title="Numerical Experience with a Class of Self-Scaling Quasi-Newton Algorithms",
	journal="Journal of Optimization Theory and Applications",
	year="1998",
	volume="96",
	number="3",
	pages="533--553",
	abstract="Self-scaling quasi-Newton methods for unconstrained optimization depend upon updating the Hessian approximation by a formula which depends on two parameters (say, $\tau$ and $\theta$) such that $\tau$ = 1, $\theta$ = 0, and $\theta$ = 1 yield the unscaled Broyden family, the BFGS update, and the DFP update, respectively. In previous work, conditions were obtained on these parameters that imply global and superlinear convergence for self-scaling methods on convex objective functions. This paper discusses the practical performance of several new algorithms designed to satisfy these conditions.",
	issn="1573-2878",
	doi="10.1023/A:1022608410710",
}

@article{Amari1993,
	title = "Backpropagation and stochastic gradient descent method ",
	journal = "Neurocomputing ",
	volume = "5",
	number = "4–5",
	pages = "185 - 196",
	year = "1993",
	note = "",
	issn = "0925-2312",
	doi = "http://doi.org/10.1016/0925-2312(93)90006-O",
	url = "http://www.sciencedirect.com/science/article/pii/092523129390006O",
	author = "Shun-ichi Amari",
	keywords = "Stochastic descent",
	keywords = "generalized delta rule",
	keywords = "dynamics of learning",
	keywords = "pattern classification",
	keywords = "multilayer perceptron "
}

@article{anderson1968,
    author = {Anderson, J. A.},
    citeulike-article-id = {3641019},
    journal = {Kybernetik},
    pages = {113--119},
    posted-at = {2008-11-21 08:28:12},
    priority = {2},
    title = {{A memory storage model utilizing spatial correlation functions}},
    volume = {5},
    year = {1968}
}

@misc{anderson1970,
    title={Two models for memory organization using interacting traces},
    DOI={10.1016/0025-5564(70)90147-1},
    journal={Math. Biosci.},
    author={J. A. Anderson},
    year={1970}
}

@article{Arel2010,
	author = {Arel, Itamar and Rose, Derek C. and Karnowski, Thomas P.},
	title = {Research Frontier: Deep Machine Learning--a New Frontier in Artificial Intelligence Research},
	journal = {Comp. Intell. Mag.},
	issue_date = {November 2010},
	volume = {5},
	number = {4},
	month = {nov},
	year = {2010},
	issn = {1556-603X},
	pages = {13--18},
	numpages = {6},
	url = {http://dx.doi.org/10.1109/MCI.2010.938364},
	doi = {10.1109/MCI.2010.938364},
	acmid = {1921920},
	publisher = {IEEE Press},
	address = {Piscataway, NJ, USA},
}

@ARTICLE{Baldi1995,
	author={P. Baldi},
	journal={IEEE Transactions on Neural Networks},
	title={Gradient descent learning algorithm overview: a general dynamical systems perspective},
	year={1995},
	volume={6},
	number={1},
	pages={182-195},
	keywords={learning (artificial intelligence);neural nets;variational techniques;adjoint methods;backpropagation,;complexity;fixed point/trajectory learning;forward architecture;general dynamical systems perspective;gradient descent learning algorithm;neural networks;recurrent architecture;trajectory learning;variational calculus;Backpropagation algorithms;Biological neural networks;Biological systems;Calculus;Context modeling;Hebbian theory;Joining processes;Neurons;Organisms;Propulsion},
	doi={10.1109/72.363438},
	ISSN={1045-9227},
	month={Jan}
}

@article{Barzilai1988,
	author = {Barzilai, Jonathan and Borwein, Jonathan M.},
	title = {Two-Point Step Size Gradient Methods},
	journal = {IMA Journal of Numerical Analysis},
	volume = {8},
	number = {1},
	pages = {141},
	year = {1988},
	doi = {10.1093/imanum/8.1.141},
	eprint = {/oup/backfile/content_public/journal/imajna/8/1/10.1093/imanum/8.1.141/2/8-1-141.pdf}
}

@inproceedings{Bengio1993,
	author = {Yoshua Bengio and Paolo Frasconi},
	title = {Credit Assignment through Time: Alternatives to Backpropagation},
	booktitle = {Advances in Neural Information Processing Systems 6, [7th {NIPS} Conference, Denver, Colorado, USA, 1993]},
	pages = {75--82},
	year = {1993},
	url = {http://papers.nips.cc/paper/724-credit-assignment-through-time-alternatives-to-backpropagation},
	timestamp = {Thu, 11 Dec 2014 17:34:08 +0100},
}

@article{Bengio1994,
	author={Y. Bengio and P. Simard and P. Frasconi},
	journal={IEEE Transactions on Neural Networks},
	title={Learning long-term dependencies with gradient descent is difficult},
	year={1994},
	volume={5},
	number={2},
	pages={157-166},
	keywords={learning (artificial intelligence);numerical analysis;recurrent neural nets;efficient learning;gradient descent;input/output sequence mapping;long-term dependencies;prediction problems;production problems;recognition;recurrent neural network training;temporal contingencies;Computer networks;Cost function;Delay effects;Discrete transforms;Displays;Intelligent networks;Neural networks;Neurofeedback;Production;Recurrent neural networks},
	doi={10.1109/72.279181},
	ISSN={1045-9227},
	month={Mar},
}

@inproceedings{Bengio2006,
	author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	title = {Greedy Layer-wise Training of Deep Networks},
	booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
	series = {NIPS'06},
	year = {2006},
	location = {Canada},
	pages = {153--160},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=2976456.2976476},
	acmid = {2976476},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@incollection{Bengio2007,
    author = {Bengio, Yoshua and LeCun, Yann},
    booktitle = {Large-Scale Kernel Machines},
    citeulike-article-id = {6095846},
    citeulike-linkout-0 = {http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf},
    editor = {Bottou, L. and Chapelle, O. and DeCoste, D. and Weston, J.},
    keywords = {convolution-networks},
    posted-at = {2009-11-10 21:47:24},
    priority = {2},
    publisher = {MIT Press},
    title = {{Scaling learning algorithms towards AI}},
    url = {http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf},
    year = {2007}
}

@article{Bengio2009,
	author = {Bengio, Yoshua},
	title = {Learning Deep Architectures for AI},
	journal = {Found. Trends Mach. Learn.},
	issue_date = {January 2009},
	volume = {2},
	number = {1},
	month = jan,
	year = {2009},
	issn = {1935-8237},
	pages = {1--127},
	numpages = {127},
	url = {http://dx.doi.org/10.1561/2200000006},
	doi = {10.1561/2200000006},
	acmid = {1658424},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
}

@article{Bengio2013,
	author={Y. Bengio and A. Courville and P. Vincent},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={Representation Learning: A Review and New Perspectives},
	year={2013},
	volume={35},
	number={8},
	pages={1798-1828},
	keywords={artificial intelligence;data structures;probability;unsupervised learning;AI;autoencoders;data representation;density estimation;geometrical connections;machine learning algorithms;manifold learning;probabilistic models;representation learning;unsupervised feature learning;Abstracts;Feature extraction;Learning systems;Machine learning;Manifolds;Neural networks;Speech recognition;Boltzmann machine;Deep learning;autoencoder;feature learning;neural nets;representation learning;unsupervised learning;Algorithms;Artificial Intelligence;Humans;Neural Networks (Computer)},
	doi={10.1109/TPAMI.2013.50},
	ISSN={0162-8828},
	month={Aug},
}

@article{Birgin2001,
	author={Birgin, E. G. and Mart{\'i}nez, J. M.},
	title={A Spectral Conjugate Gradient Method for Unconstrained Optimization},
	journal={Applied Mathematics and Optimization},
	year={2001},
	volume={43},
	number={2},
	pages={117--128},
	abstract={A family of scaled conjugate gradient algorithms for large-scale unconstrained minimization is defined. The Perry, the Polak---Ribi{\`e}re and the Fletcher---Reeves formulae are compared using a spectral scaling derived from Raydan's spectral gradient optimization method. The best combination of formula, scaling and initial choice of step-length is compared against well known algorithms using a classical set of problems. An additional comparison involving an ill-conditioned estimation problem in Optics is presented.},
	issn={1432-0606},
	doi={10.1007/s00245-001-0003-0},
	url={http://dx.doi.org/10.1007/s00245-001-0003-0}
}

@article{Cauchy1847,
    author = {Cauchy, Augustin-Louis},
    citeulike-article-id = {7133110},
    day = {18},
    journal = {Compte Rendu des S'eances de L'Acad'emie des Sciences XXV},
    keywords = {first, steepest-descent},
    month = oct,
    number = {25},
    pages = {536--538},
    posted-at = {2010-05-07 01:16:11},
    priority = {2},
    title = {{M\'{e}thode g\'{e}n\'{e}rale pour la r\'{e}solution des syst\`{e}mes d'\'{e}quations simultan\'{e}es}},
    volume = {S'erie A},
    year = {1847}
}

@article{Charalambous1992,
	author={C. Charalambous},
	journal={IEE Proceedings G - Circuits, Devices and Systems},
	title={Conjugate gradient algorithm for efficient training of artificial neural networks},
	year={1992},
	volume={139},
	number={3},
	pages={301-310},
	keywords={conjugate gradient methods;learning systems;neural nets;artificial neural networks;conjugate gradient algorithm;line search algorithm;multilayer feedforward neural networks;Conjugate gradient methods;Learning systems;Neural networks},
	doi={10.1049/ip-g-2.1992.0050},
	ISSN={0956-3768},
	month={June},
}

@article{Ciresan2012,
	title = "Multi-column deep neural network for traffic sign classification ",
	journal = "Neural Networks ",
	volume = "32",
	number = "",
	pages = "333 - 338",
	year = "2012",
	note = "Selected Papers from \{IJCNN\} 2011 ",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2012.02.023",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608012000524",
	author = "Dan Cireşan and Ueli Meier and Jonathan Masci and Jürgen Schmidhuber",
	keywords = "Deep neural networks",
	keywords = "Image classification",
	keywords = "Traffic signs",
	keywords = "Image preprocessing ",
	abstract = "We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46%. We use a fast, fully parameterizable \{GPU\} implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various \{DNNs\} trained on differently preprocessed data into a Multi-Column \{DNN\} (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination. "
}
@Book{Cruz2011,
    author = {Cruz, Pedro},
    title = {Inteligencia artificial con aplicaciones a la ingeniería},
    publisher = {Marcombo},
    year = {2011},
    address = {Barcelona},
    isbn = {978-8426717061}
}

@article{Deng2014,
	url = {http://dx.doi.org/10.1561/2000000039},
	year = {2014},
	volume = {7},
	journal = {Foundations and Trends® in Signal Processing},
	title = {Deep Learning: Methods and Applications},
	doi = {10.1561/2000000039},
	issn = {1932-8346},
	number = {3–4},
	pages = {197-387},
	author = {Li Deng and Dong Yu}
}

@article{Elman1990,
	title = "Finding structure in time ",
	journal = "Cognitive Science ",
	volume = "14",
	number = "2",
	pages = "179 - 211",
	year = "1990",
	note = "",
	issn = "0364-0213",
	doi = "https://doi.org/10.1016/0364-0213(90)90002-E",
	url = "http://www.sciencedirect.com/science/article/pii/036402139090002E",
	author = "Jeffrey L. Elman",
	abstract = "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
}

@article{Fletcher1964,
    abstract = {{A quadratically convergent gradient method for locating an unconstrained local minimum of a function of several variables is described. Particular advantages are its simplicity and its modest demands on storage, space for only three vectors being required. An ALGOL procedure is presented, and the paper includes a discussion of results obtained by its used on various test functions. 10.1093/comjnl/7.2.149}},
    author = {Fletcher, R. and Reeves, C. M.},
    citeulike-article-id = {7133131},
    citeulike-linkout-0 = {http://dx.doi.org/10.1093/comjnl/7.2.149},
    day = {1},
    doi = {10.1093/comjnl/7.2.149},
    journal = {The Computer Journal},
    keywords = {conjugate-gradient},
    month = feb,
    number = {2},
    pages = {149--154},
    posted-at = {2010-05-07 01:16:26},
    priority = {2},
    title = {{Function minimization by conjugate gradients}},
    url = {http://dx.doi.org/10.1093/comjnl/7.2.149},
    volume = {7},
    year = {1964}
}

@mastersthesis{Fritsch1996,
	Title	= {Modular Neural Networks for Speech Recognition},
	Author	= {Jürgen Fritsch},
	School	= {KIT},
	Year	= {1996},
	Month	= {July 31},
	Type	= {Masters Thesis},
	File	= {:Fritsch1996b.pdf:PDF},
	Link	= {http://isl.anthropomatik.kit.edu/cmu-kit/english/4894.php},
}

@article{Glauner2015,
	author    = {Patrick O. Glauner},
	title     = {Comparison of Training Methods for Deep Neural Networks},
	journal   = {CoRR},
	volume    = {abs/1504.06825},
	year      = {2015},
	url       = {http://arxiv.org/abs/1504.06825},
	timestamp = {Sat, 02 May 2015 17:50:32 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Glauner15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Goh1997,
	author="Goh, B. S.",
	title="Algorithms for Unconstrained Optimization Problems via Control Theory",
	journal="Journal of Optimization Theory and Applications",
	year="1997",
	volume="92",
	number="3",
	pages="581--604",
	abstract="Existing algorithms for solving unconstrained optimization problems are generally only optimal in the short term. It is desirable to have algorithms which are long-term optimal. To achieve this, the problem of computing the minimum point of an unconstrained function is formulated as a sequence of optimal control problems. Some qualitative results are obtained from the optimal control analysis. These qualitative results are then used to construct a theoretical iterative method and a new continuous-time method for computing the minimum point of a nonlinear unconstrained function. New iterative algorithms which approximate the theoretical iterative method and the proposed continuous-time method are then established. For convergence analysis, it is useful to note that the numerical solution of an unconstrained optimization problem is none other than an inverse Lyapunov function problem. Convergence conditions for the proposed continuous-time method and iterative algorithms are established by using the Lyapunov function theorem.",
	issn="1573-2878",
	doi="10.1023/A:1022607507153",
}

@article{Graves2013,
	author    = {Alex Graves},
	title     = {Generating Sequences With Recurrent Neural Networks},
	journal   = {CoRR},
	volume    = {abs/1308.0850},
	year      = {2013},
	url       = {http://arxiv.org/abs/1308.0850},
	timestamp = {Tue, 10 Dec 2013 12:03:02 +0100},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graves13},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Grippo1994,
	author = {L. Grippo},
	title = {A class of unconstrained minimization methods for neural network training},
	journal = {Optimization Methods and Software},
	volume = {4},
	number = {2},
	pages = {135-150},
	year = {1994},
	doi = {10.1080/10556789408805583},
}

@article{Gori1992,
	author={M. Gori and A. Tesi},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={On the problem of local minima in backpropagation},
	year={1992},
	volume={14},
	number={1},
	pages={76-86},
	keywords={learning systems;neural nets;pattern recognition;backpropagation;convergence;learning systems;local minima;multilayered neural networks;network architecture;pattern recognition;perceptrons;Backpropagation algorithms;Convergence;Gradient methods;Intelligent networks;Multi-layer neural network;Neural networks;Neurons;Pattern recognition;Speech recognition;Workstations},
	doi={10.1109/34.107014},
	ISSN={0162-8828},
	month={Jan},
}

@article{Hagan1994,
	author={M. T. Hagan and M. B. Menhaj},
	journal={IEEE Transactions on Neural Networks},
	title={Training feedforward networks with the Marquardt algorithm},
	year={1994},
	volume={5},
	number={6},
	pages={989-993},
	keywords={backpropagation;feedforward neural nets;function approximation;least squares approximations;Marquardt algorithm;backpropagation;feedforward network training;feedforward neural networks;function approximation;learning;nonlinear least squares;Acceleration;Approximation algorithms;Backpropagation algorithms;Convergence;Feedforward neural networks;Function approximation;Least squares approximation;Least squares methods;Neural networks;Testing},
	doi={10.1109/72.329697},
	ISSN={1045-9227},
	month={Nov},
}

@article{He2015,
	author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	timestamp = {Wed, 30 Mar 2016 23:40:00 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HeZRS15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@book{hebb2002,
    title={The Organization of Behavior: A Neuropsychological Theory},
    author={Hebb, D.O.},
    isbn={9781410612403},
    series={E-Books von NetLibrary},
    year={2002},
    publisher={Taylor \& Francis}
}

@article{Hestenes1952,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {Hestenes, M. R. and Stiefel, E.},
	biburl = {https://www.bibsonomy.org/bibtex/2362a03244482b96f9f9ee4af6eb7f80c/brefeld},
	interhash = {bcb34a6f8b9fb2f2371e92430116a8ad},
	intrahash = {362a03244482b96f9f9ee4af6eb7f80c},
	journal = {Journal of research of the National Bureau of Standards},
	keywords = {imported},
	pages = {409--436},
	timestamp = {2008-10-07T16:03:39.000+0200},
	title = {Methods of conjugate gradients for solving linear systems},
	volume = {49},
	year = {1952}
}

@article{Hinton2006,
	author = { Geoffrey E. Hinton and  Simon Osindero and  Yee-Whye Teh},
	title = {A Fast Learning Algorithm for Deep Belief Nets},
	journal = {Neural Computation},
	volume = {18},
	number = {7},
	pages = {1527-1554},
	year = {2006},
	doi = {10.1162/neco.2006.18.7.1527},
	note ={PMID: 16764513},
	URL = { http://dx.doi.org/10.1162/neco.2006.18.7.1527},
	eprint = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
	abstract = { We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
}

@article{hopfield1982,
    abstract = {{Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}},
    author = {Hopfield, J. J.},
    citeulike-article-id = {878138},
    citeulike-linkout-0 = {http://www.pnas.org/content/79/8/2554.abstract},
    citeulike-linkout-1 = {http://www.pnas.org/content/79/8/2554.full.pdf},
    citeulike-linkout-2 = {http://www.pnas.org/cgi/content/abstract/79/8/2554},
    citeulike-linkout-3 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=6953413},
    day = {1},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    keywords = {emergence, neural\_computation},
    month = apr,
    number = {8},
    pages = {2554--2558},
    pmcid = {PMC346238},
    pmid = {6953413},
    posted-at = {2009-10-06 16:50:26},
    priority = {4},
    publisher = {National Academy of Sciences},
    title = {{Neural networks and physical systems with emergent collective computational abilities.}},
    volume = {79},
    year = {1982}
}

@masterthesis {Hochreiter1991,
    author = "Sepp Hochreiter",
    title  = "Untersuchungen zu dynamischen neuronalen Netzen",
    school = "Institut f. Informatik, Technische Univ. Munich",
    year   = "1991",
    type   = "Diploma thesis"
}

@article{Hochreiter1997a,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    journal = {Neural Comput.},
    issue_date = {November 15, 1997},
    volume = {9},
    number = {8},
    month = nov,
    year = {1997},
    issn = {0899-7667},
    pages = {1735--1780},
    numpages = {46},
    url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    acmid = {1246450},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
}

@article{Hochreiter1998b,
 author = {Hochreiter, Sepp},
 title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
 journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
 issue_date = {April 1998},
 volume = {6},
 number = {2},
 month = apr,
 year = {1998},
 issn = {0218-4885},
 pages = {107--116},
 numpages = {10},
 url = {http://dx.doi.org/10.1142/S0218488598000094},
 doi = {10.1142/S0218488598000094},
 acmid = {355233},
 publisher = {World Scientific Publishing Co., Inc.},
 address = {River Edge, NJ, USA},
 keywords = {long short-term memory, long-term dependencies, recurrent neural nets, vanishing gradient},
}

@article{Huang1970,
	author="Huang, H. Y.",
	title="Unified approach to quadratically convergent algorithms for function minimization",
	journal="Journal of Optimization Theory and Applications",
	year="1970",
	volume="5",
	number="6",
	pages="405--423",
	abstract="In this paper, a unified method to construct quadratically convergent algorithms for function minimization is described. With this unified method, a generalized algorithm is derived. It is shown that all the existing conjugate-gradient algorithms and variable-metric algorithms can be obtained as particular cases. In addition, several new practical algorithms can be generated. The application of these algorithms to quadratic functions as well as nonquadratic functions is discussed.",
	issn="1573-2878",
	doi="10.1007/BF00927440",
	url="http://dx.doi.org/10.1007/BF00927440"
}

@article{Huang2005,
	author={Guang-Bin Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation},
	year={2005},
	volume={16},
	number={1},
	pages={57-67},
	keywords={function approximation;learning (artificial intelligence);radial basis function networks;function approximation;growing RBF neural network;pruning RBF neural network;radial basis function networks;sequential learning algorithm;Approximation algorithms;Backpropagation algorithms;Function approximation;Joining processes;Learning systems;Neural networks;Neurons;Radial basis function networks;Sampling methods;Training data;Growing;neuron's significance;pruning;radial basis networks;sequential learning;Algorithms;Artificial Intelligence;Cluster Analysis;Computing Methodologies;Neural Networks (Computer);Numerical Analysis, Computer-Assisted},
	doi={10.1109/TNN.2004.836241},
	ISSN={1045-9227},
	month={Jan},
}

@article{Jacobs1988,
	title = "Increased rates of convergence through learning rate adaptation ",
	journal = "Neural Networks ",
	volume = "1",
	number = "4",
	pages = "295 - 307",
	year = "1988",
	note = "",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/0893-6080(88)90003-2",
	author = "Robert A. Jacobs",
	abstract = "While there exist many techniques for finding the parameters that minimize an error function, only those methods that solely perform local computations are used in connectionist networks. The most popular learning algorithm for connectionist networks is the back-propagation procedure, which can be used to update the weights by the method of steepest descent. In this paper, we examine steepest descent and analyze why it can be slow to converge. We then propose four heuristics for achieving faster rates of convergence while adhering to the locality constraint. These heuristics suggest that every weight of a network should be given its own learning rate and that these rates should be allowed to vary over time. Additionally, the heuristics suggest how the learning rates should be adjusted. Two implementations of these heuristics, namely momentum and an algorithm called the delta-bar-delta rule, are studied and simulation results are presented. "
}

@misc{Keras2015,
	title={Keras},
	author={Chollet, Fran\c{c}ois},
	year={2015},
	publisher={GitHub},
	howpublished={\url{https://github.com/fchollet/keras}},
}

@article{kohonen1972,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={Correlation Matrix Memories},
    year={1972},
    month={April},
    volume={C-21},
    number={4},
    pages={353-359},
    keywords={Associative memory;Biological information theory;Biomedical optical imaging;Encoding;Holographic optical components;Holography;Mathematical model;Sampling methods;Signal analysis;Stochastic processes;Associative memory;associative net;associative recall;correlation matrix memory;nonholographic associative memory;pattern recognition},
    doi={10.1109/TC.1972.5008975},
    ISSN={0018-9340},
}

@article{kohonen1974,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={An Adaptive Associative Memory Principle},
    year={1974},
    month={April},
    volume={C-23},
    number={4},
    pages={444-445},
    keywords={Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.;Associative memory;Circuits;Distributed computing;Gradient methods;Information systems;Learning systems;Pattern recognition;Physics computing;Quantum computing;Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.},
    doi={10.1109/T-C.1974.223960},
    ISSN={0018-9340},
}

@inproceedings{Le2012,
author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
title = {Building High-level Features Using Large Scale Unsupervised Learning},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
series = {ICML'12},
year = {2012},
isbn = {978-1-4503-1285-1},
location = {Edinburgh, Scotland},
pages = {507--514},
numpages = {8},
url = {http://dl.acm.org/citation.cfm?id=3042573.3042641},
acmid = {3042641},
publisher = {Omnipress},
address = {USA},
}

@article{LeCun2015,
	doi = {10.1038/nature14539},
	url = {https://doi.org/10.1038/nature14539},
	year  = {2015},
	month = {may},
	publisher = {Springer Nature},
	volume = {521},
	number = {7553},
	pages = {436--444},
	author = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
	title = {Deep learning},
	journal = {Nature}
}

@article{Leonard1990,
	title = "Improvement of the backpropagation algorithm for training neural networks",
	journal ={"Computers \& Chemical Engineering},
	volume = "14",
	number = "3",
	pages = "337 - 341",
	year = "1990",
	note = "",
	issn = "0098-1354",
	doi = "http://dx.doi.org/10.1016/0098-1354(90)87070-6",
	url = "http://www.sciencedirect.com/science/article/pii/0098135490870706",
	author = "J. Leonard and M.A. Kramer",
}

@article{Levenberg1944,
    author = {Levenberg, Kenneth},
    citeulike-article-id = {10796881},
    journal = {Quarterly Journal of Applied Mathmatics},
    keywords = {indefinite, nonconvex, optimization},
    number = {2},
    pages = {164--168},
    posted-at = {2012-06-17 09:00:21},
    priority = {2},
    title = {{A method for the solution of certain non-linear problems in least squares}},
    volume = {II},
    year = {1944}
}

@article{Lian2006,
	author={N. y. Liang and G. b. Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks},
	year={2006},
	volume={17},
	number={6},
	pages={1411-1423},
	keywords={learning (artificial intelligence);radial basis function networks;additive hidden mode;batch learning;bounded nonconstant piecewise continuous functions;online sequential extreme learning machine;radial basis function hidden mode;single hidden layer feedforward networks;Convergence;Industrial training;Machine learning;Neural networks;Radial basis function networks;Radio access networks;Resource management;Spine;Stochastic processes;Training data;Extreme learning machine (ELM);GGAP-RBF;growing and pruning RBF network (GAP-RBF);minimal resource allocation network (MRAN);online sequential ELM (OS-ELM);resource allocation network (RAN);resource allocation network via extended kalman filter (RANEKF);stochastic gradient descent back-propagation (SGBP);Algorithms;Information Storage and Retrieval;Information Theory;Neural Networks (Computer);Online Systems;Pattern Recognition, Automated;Signal Processing, Computer-Assisted},
	doi={10.1109/TNN.2006.880583},
	ISSN={1045-9227},
	month={Nov},
}

@article{Lipton2015,
  author    = {Zachary Chase Lipton},
  title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1506.00019},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.00019},
  timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Lipton15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{MacKay1992a,
	author = {MacKay, David J. C.},
	title = {Bayesian Interpolation},
	journal = {Neural Comput.},
	issue_date = {May 1992},
	volume = {4},
	number = {3},
	month = may,
	year = {1992},
	issn = {0899-7667},
	pages = {415--447},
	numpages = {33},
	url = {http://dx.doi.org/10.1162/neco.1992.4.3.415},
	doi = {10.1162/neco.1992.4.3.415},
	acmid = {148163},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@article{MacKay1992b,
	author={D. J. C. MacKay},
	journal={Neural Computation},
	title={A Practical Bayesian Framework for Backpropagation Networks},
	year={1992},
	volume={4},
	number={3},
	pages={448-472},
	doi={10.1162/neco.1992.4.3.448},
	ISSN={0899-7667},
	month={May},
}

@article{Magoulas1997,
	title = "Effective Backpropagation Training with Variable Stepsize ",
	journal = "Neural Networks ",
	volume = "10",
	number = "1",
	pages = "69 - 82",
	year = "1997",
	note = "",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/S0893-6080(96)00052-4",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608096000524",
	author = "George D. Magoulas and Michael N. Vrahatis and George S. Androulakis",
	keywords = "Feedforward neural network",
	keywords = "Numerical optimization techniques",
	keywords = "Steepest descent",
	keywords = "Error backpropagation ",
	abstract = "The issue of variable stepsize in the backpropagation training algorithm has been widely investigated and several techniques employing heuristic factors have been suggested to improve training time and reduce convergence to local minima. In this contribution, backpropagation training is based on a modified steepest descent method which allows variable stepsize. It is computationally efficient and posseses interesting convergence properties utilizing estimates of the Lipschitz constant without any additional computational cost. The algorithm has been implemented and tested on several problems and the results have been very satisfactory. Numerical evidence shows that the method is robust with good average performance on many classes of problems. Copyright © 1996 Elsevier Science Ltd. "
}

@article{Marquardt1963,
	author = {Donald W. Marquardt},
	title = {An Algorithm for Least-Squares Estimation of Nonlinear Parameters},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	volume = {11},
	number = {2},
	pages = {431-441},
	year = {1963},
	doi = {10.1137/0111030},
	URL = { http://dx.doi.org/10.1137/0111030},
	eprint = {http://dx.doi.org/10.1137/0111030}
}

@article{McCulloch1943,
	author="McCulloch, Warren S. and Pitts, Walter",
	title="A logical calculus of the ideas immanent in nervous activity",
	journal="The bulletin of mathematical biophysics",
	year="1943",
	volume="5",
	number="4",
	pages="115--133",
	abstract="Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
	issn="1522-9602",
	doi="10.1007/BF02478259",
}

@article{mcculloch_pitts1943,
    year={1943},
    issn={0007-4985},
    journal={The bulletin of mathematical biophysics},
    volume={5},
    number={4},
    doi={10.1007/BF02478259},
    title={A logical calculus of the ideas immanent in nervous activity},
    publisher={Kluwer Academic Publishers},
    author={McCulloch, WarrenS. and Pitts, Walter},
    pages={115-133},
    language={English}
}

@article {Minsky1969,
	author = {Newell, Allen},
	title = {Perceptrons: An Introduction to Computational Geometry. Marvin Minsky and Seymour Papert. M.I.T. Press, Cambridge, Mass., 1969. vi + 258 pp., illus. Cloth, $12; paper, $4.95},
	volume = {165},
	number = {3895},
	pages = {780--782},
	year = {1969},
	doi = {10.1126/science.165.3895.780},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/165/3895/780},
	eprint = {http://science.sciencemag.org/content/165/3895/780.full.pdf},
	journal = {Science}
}

@book{Minsky1969b,
	title = {Perceptrons: An Introduction to Computational Geometry},
	author = {Marvin Minsky, Seymour A. Papert},
	publisher = {The MIT Press},
	isbn = {0262631113,9780262631112},
	year = {1987},
	series = {},
	edition = {Expanded},
	volume = {},
}

@article{Moller1993,
	title = "A scaled conjugate gradient algorithm for fast supervised learning ",
	journal = "Neural Networks ",
	volume = "6",
	number = "4",
	pages = "525 - 533",
	year = "1993",
	note = "",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/S0893-6080(05)80056-5",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608005800565",
	author = "Martin Fodslette Møller",
	keywords = "Feedforward neural network",
	keywords = "Supervised learning",
	keywords = "Optimization",
	keywords = "Conjugate gradient algorithms ",
	abstract = "A supervised learning algorithm (Scaled Conjugate Gradient, SCG) is introduced. The performance of \{SCG\} is benchmarked against that of the standard back propagation algorithm (BP) (Rumelhart, Hinton, &amp; Williams, 1986), the conjugate gradient algorithm with line search (CGL) (Johansson, Dowla, &amp; Goodman, 1990) and the one-step Broyden-Fletcher-Goldfarb-Shanno memoriless quasi-Newton algorithm (BFGS) (Battiti, 1990). \{SCG\} is fully-automated, includes no critical user-dependent parameters, and avoids a time consuming line search, which \{CGL\} and \{BFGS\} use in each iteration in order to determine an appropriate step size. Experiments show that \{SCG\} is considerably faster than BP, CGL, and BFGS. "
}

@inproceedings{Morse2016,
	author = {Morse, Gregory and Stanley, Kenneth O.},
	title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	series = {GECCO '16},
	year = {2016},
	isbn = {978-1-4503-4206-3},
	location = {Denver, Colorado, USA},
	pages = {477--484},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/2908812.2908916},
	doi = {10.1145/2908812.2908916},
	acmid = {2908916},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
}

@inproceedings{Nguyen1990,
	author={D. Nguyen and B. Widrow},
	booktitle={1990 IJCNN International Joint Conference on Neural Networks},
	title={Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights},
	year={1990},
	pages={21-26 vol.3},
	keywords={adaptive systems;learning systems;neural nets;2-layer neural networks;adaptive weights;complicated desired response;hidden units;initial weights;learning speed;nonlinear function;piecewise linear segments;training problems;training time;truck-backer-upper;two-layer neural network},
	doi={10.1109/IJCNN.1990.137819},
	month={June},
}

@article{Nocedal1993,
	author="Nocedal, Jorge
	and Yuan, Ya-xiang",
	title="Analysis of a self-scaling quasi-Newton method",
	journal="Mathematical Programming",
	year="1993",
	volume="61",
	number="1",
	pages="19--37",
	abstract="We study the self-scaling BFGS method of Oren and Luenberger (1974) for solving unconstrained optimization problems. For general convex functions, we prove that the method is globally convergent with inexact line searches. We also show that the directions generated by the self-scaling BFGS method approach Newton's direction asymptotically. This would ensure superlinear convergence if, in addition, the search directions were well-scaled, but we show that this is not always the case. We find that the method has a major drawback: to achieve superlinear convergence it may be necessary to evaluate the function twice per iteration, even very near the solution. An example is constructed to show that the step-sizes required to achieve a superlinear rate converge to 2 and 0.5 alternately.",
	issn="1436-4646",
	doi="10.1007/BF01582136",
}

@book{Nocedal2006,
	author = {Jorge Nocedal and Stephen Wright},
	Title = {Numerical Optimization (Springer Series in Operations Research and Financial Engineering)},
	Publisher = {Springer},
	Year = {2006},
	isbn = {978-0-387-30303-1}
}

@book{Oren1972,
	title={Self-scaling Variable Metric Algorithms for Unconstrained Minimization},
	author={Oren, S.S.},
	year={1972},
	publisher={Department of Engineering-Economic Systems, Stanford University.}
}

@article{Oren1974,
	ISSN = {00251909, 15265501},
	URL = {http://www.jstor.org/stable/2630094},
	abstract = {A new criterion is introduced for comparing the convergence properties of variable metric algorithms, focusing on stepwise descent properties. This criterion is a bound on the rate of decrease in the function value at each iterative step (single-step convergence rate). Using this criterion as a basis for algorithm development leads to the introduction of variable coefficients to rescale the objective function at each iteration, and, correspondingly, to a new class of variable metric algorithms. Effective scaling can be implemented by restricting the parameters in a two-parameter family of variable metric algorithms. Conditions are derived for these parameters that guarantee monotonic improvement in the single-step convergence rate. These conditions are obtained by analyzing the eigenvalue structure of the associated inverse Hessian approximations.},
	author = {Shmuel S. Oren and David G. Luenberger},
	journal = {Management Science},
	number = {5},
	pages = {845-862},
	publisher = {INFORMS},
	title = {Self-Scaling Variable Metric (SSVM) Algorithms. Part I: Criteria and Sufficient Conditions for Scaling a Class of Algorithms},
	volume = {20},
	year = {1974}
}

@article{Pearlmutter1989,
author={B. A. Pearlmutter},
journal={Neural Computation},
title={Learning State Space Trajectories in Recurrent Neural Networks},
year={1989},
volume={1},
number={2},
pages={263-269},
doi={10.1162/neco.1989.1.2.263},
ISSN={0899-7667},
month={June},
}

@article{Pearlmutter1995,
author={B. A. Pearlmutter},
journal={IEEE Transactions on Neural Networks},
title={Gradient calculations for dynamic recurrent neural networks: a survey},
year={1995},
volume={6},
number={5},
pages={1212-1228},
keywords={Boltzmann machines;backpropagation;recurrent neural nets;Elman's history cutoff;Jordan's output feedback architecture;backpropagation through time;computational complexity;deterministic Boltzmann machines;dynamic recurrent neural networks;fixed point learning algorithms;forward propagation;gradient calculations;learning speed;nonfixed point algorithms;recurrent backpropagation;temporally continuous neural networks;Backpropagation algorithms;Clocks;Computational complexity;Computational modeling;Equations;History;Machine learning;Neural networks;Output feedback;Recurrent neural networks},
doi={10.1109/72.410363},
ISSN={1045-9227},
month={Sep},
}

@inproceedings{Peng2007,
	author={C. C. Peng and G. D. Magoulas},
	booktitle={19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)},
	title={Adaptive Nonmonotone Conjugate Gradient Training Algorithm for Recurrent Neural Networks},
	year={2007},
	volume={2},
	pages={374-381},
	keywords={conjugate gradient methods;feedforward neural nets;learning (artificial intelligence);recurrent neural nets;adaptive nonmonotone conjugate gradient training algorithm;adaptive tuning strategy;feedforward network;nonmonotone learning horizon;recurrent neural network;Artificial intelligence;Computer science;Delay effects;Educational institutions;Equations;Feedforward neural networks;Information systems;Neural networks;Power system modeling;Recurrent neural networks},
	doi={10.1109/ICTAI.2007.126},
	ISSN={1082-3409},
	month={Oct},
}

@article{Plagianakos1998,
	title = {Automatic adaptation of learning rate for backpropagation neural networks},
	author={Plagianakos, VP and Sotiropoulos, DG and Vrahatis, MN},
	journal={Recent Advances in Circuits and Systems},
	volume={337},
	year={1998},
	publisher={Singapore: World Scientific},
}

@article{Plagianakos2002,
	author={V. P. Plagianakos and G. D. Magoulas and M. N. Vrahatis},
	journal={IEEE Transactions on Neural Networks},
	title={Deterministic nonmonotone strategies for effective training of multilayer perceptrons},
	year={2002},
	volume={13},
	number={6},
	pages={1268-1284},
	keywords={convergence;feedforward neural nets;learning (artificial intelligence);multilayer perceptrons;adaptive learning rate algorithms;batch training algorithm;convergence;deterministic nonmonotone learning;deterministic training algorithms;error function values;experimental results;fine-tuning;first-order training algorithms;heuristic parameters;maximum error function value;multilayer perceptrons;Artificial intelligence;Backpropagation algorithms;Convergence;Helium;Information systems;Iterative algorithms;Mathematics;Minimization methods;Multilayer perceptrons;Numerical analysis},
	doi={10.1109/TNN.2002.804225},
	ISSN={1045-9227},
	month={Nov},
}

@article{Polak1969,
	author = {Polak E., Ribiere G.},
	journal = {ESAIM: Mathematical Modelling and Numerical Analysis - Modélisation Mathématique et Analyse Numérique},
	keywords = {numerical analysis},
	language = {fre},
	number = {R1},
	pages = {35-43},
	publisher = {Dunod},
	title = {Note sur la convergence de méthodes de directions conjuguées},
	url = {http://eudml.org/doc/193115},
	volume = {3},
	year = {1969},
}

@article{Puskorius1994,
	author={G. V. Puskorius and L. A. Feldkamp},
	journal={IEEE Transactions on Neural Networks},
	title={Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks},
	year={1994},
	volume={5},
	number={2},
	pages={279-297},
	keywords={Kalman filters;filtering and prediction theory;nonlinear control systems;nonlinear dynamical systems;recurrent neural nets;Kalman filter trained recurrent networks;automotive subsystem;bioreactor benchmark problems;cart-pole;decoupled extended Kalman filter;engine idle speed control;homogeneous architecture;neurocontrol;nonlinear dynamical systems;parameter-based extended Kalman filter algorithms;state-space controllers;state-space observers;Automotive engineering;Bioreactors;Control systems;Engines;Neural networks;Noise measurement;Nonlinear control systems;Nonlinear dynamical systems;Recurrent neural networks;Velocity control},
	doi={10.1109/72.279191},
	ISSN={1045-9227},
	month={Mar},
}

@incollection{Ranzato2007,
	Publisher = {MIT Press},
	Author = {Marc'aurelio Ranzato and Y-lan Boureau and Yann L. Cun},
	Url = {http://books.nips.cc/papers/files/nips20/NIPS2007_1118.pdf},
	Booktitle = {Advances in Neural Information Processing Systems 20},
	Title = {Sparse Feature Learning for Deep Belief Networks},
	Year = {2007},
	Editor = {J.c. Platt and D. Koller and Y. Singer and S. Roweis},
	Address = {Cambridge, MA},
	Pages = {1185--1192}
}

@inproceedings{Riedmiller1993,
	author={M. Riedmiller and H. Braun},
	booktitle={IEEE International Conference on Neural Networks},
	title={A direct adaptive method for faster backpropagation learning: the RPROP algorithm},
	year={1993},
	pages={586-591 vol.1},
	keywords={adaptive systems;backpropagation;feedforward neural nets;RPROP algorithm;direct adaptive method;error function;faster backpropagation learning;gradient decent type;multilayer feedforward networks;neural nets;weight-updates;Acceleration;Backpropagation algorithms;Computer networks;Convergence;Feedforward systems;Neurons;Supervised learning;Writing},
	doi={10.1109/ICNN.1993.298623},
	month={},
}

@article{Rios2013,
	author="Rios, Luis Miguel
	and Sahinidis, Nikolaos V.",
	title="Derivative-free optimization: a review of algorithms and comparison of software implementations",
	journal="Journal of Global Optimization",
	year="2013",
	volume="56",
	number="3",
	pages="1247--1293",
	abstract="This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.",
	issn="1573-2916",
	doi="10.1007/s10898-012-9951-y",
	url="http://dx.doi.org/10.1007/s10898-012-9951-y"
}

@article{Robitaille1996,
	title = "Modified quasi-Newton methods for training neural networks",
	journal = "Computers \& Chemical Engineering",
	volume = "20",
	number = "9",
	pages = "1133 - 1140",
	year = "1996",
	note = "",
	issn = "0098-1354",
	doi = "http://dx.doi.org/10.1016/0098-1354(95)00228-6",
	url = "http://www.sciencedirect.com/science/article/pii/0098135495002286",
	author = "B. Robitaille and B. Marcos and M. Veillette and G. Payre",
}

@techreport{rosenblatt1957,
    author= {F. Rosenblatt},
    title= {The perceptron---a perceiving and recognizing automaton},
    type= {Report},
    number= {85-460-1},
    institution= {Cornell Aeronautical Laboratory},
    year= 1957,
    comment= PRNNref,
}

@book{rosenblatt1958,
    title={The perceptron: a theory of statistical separability in cognitive systems (Project Para)},
    author={Rosenblatt, F. and Cornell Aeronautical Laboratory},
    series={Report},
    year={1958},
    publisher={Cornell Aeronautical Laboratory}
}

@book{Rosenblatt1962,
	title={Principles of neurodynamics: perceptrons and the theory of brain mechanisms},
	author={Rosenblatt, F.},
	lccn={62012882},
	series={Report (Cornell Aeronautical Laboratory)},
	year={1962},
	publisher={Spartan Books}
}

@article{Rumelhart1986,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	journal={Nature},
	volume={323},
	pages={533--536},
	year={1986}
}

@incollection{Rumelhart1986b,
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	chapter = {Learning Internal Representations by Error Propagation},
	title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
	editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
	 year = {1986},
	isbn = {0-262-68053-X},
	pages = {318--362},
	numpages = {45},
	url = {http://dl.acm.org/citation.cfm?id=104279.104293},
	acmid = {104293},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@Book{Sampieri2006,
    author = {Sampieri, Roberto},
    title = {Metodolog\'{i}a de la investigaci\'{o}n},
    publisher = {McGraw Hill},
    year = {2006},
    address = {M\'{e}xico},
    isbn = {970-10-5753-8}
}

@article{Schmidhuber1992a,
	author = {Schmidhuber, J\"{u}rgen},
	title = {Learning Complex, Extended Sequences Using the Principle of History Compression},
	journal = {Neural Comput.},
	issue_date = {March 1992},
	volume = {4},
	number = {2},
	month = {mar},
	year = {1992},
	issn = {0899-7667},
	pages = {234--242},
	numpages = {9},
	url = {http://dx.doi.org/10.1162/neco.1992.4.2.234},
	doi = {10.1162/neco.1992.4.2.234},
	acmid = {148076},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@article{Schmidhuber1992b,
	title={A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks},
	author={J\''{u}rgen Schmidhuber},
	journal={Neural Computation},
	year={1992},
	volume={4},
	pages={243-248},
}

@techreport{Schmidhuber1996,
    author = {J{\"u}rgen Schmidhuber and Sepp Hochreiter},
    title = {Guessing Can Outperform Many Long Time Lag Algorithms},
    institution = {},
    year = {1996}
}

@inproceedings{Sotiropoulos2002,
	title={A spectral version of Perry’s conjugate gradient method for neural network training},
	author={Sotiropoulos, DG and Kostopoulos, AE and Grapsa, TN},
	booktitle={Proceedings of 4th GRACM Congress on Computational Mechanics},
	volume={1},
	pages={291--298},
	year={2002},
	organization={Citeseer}
}
@INPROCEEDINGS{Squartini2003a,
	author={S. Squartini and A. Hussain and F. Piazza},
	booktitle={Circuits and Systems, 2003. ISCAS '03. Proceedings of the 2003 International Symposium on},
	title={Preprocessing based solution for the vanishing gradient problem in recurrent neural networks},
	year={2003},
	volume={5},
	pages={V-713-V-716 vol.5},
	keywords={discrete wavelet transforms;gradient methods;learning (artificial intelligence);recurrent neural nets;time series;DWT;RMN;RNN learning;RNN signal preprocessing;frequency scales;latching problem;learning techniques;network architecture;partial result combination;recurrent multiscale network;recurrent neural networks;short/long term information separation;time scales;time series prediction;vanishing gradient problem;wavelet decomposition;Computer architecture;Computer networks;Delay effects;Digital signal processing;Electronic mail;Information analysis;Intelligent networks;Neural networks;Recurrent neural networks;Signal processing algorithms},
	doi={10.1109/ISCAS.2003.1206412},
	month={May},
}

@INPROCEEDINGS{Squartini2003b,
	author={S. Squartini and A. Hussain and F. Piazza},
	booktitle={Proceedings of the International Joint Conference on Neural Networks, 2003.},
	title={Attempting to reduce the vanishing gradient effect through a novel recurrent multiscale architecture},
	year={2003},
	volume={4},
	pages={2819-2824 vol.4},
	keywords={learning (artificial intelligence);neural net architecture;recurrent neural nets;signal processing;time series;wavelet transforms;adaptive nonlinear structure;discrete wavelet decomposition;latching problem;long term dependencies detection;long term information;recurrent multiscale architecture;short term information;time series prediction;vanishing gradient effect;Computer architecture;Computer hacking;Delay effects;Discrete wavelet transforms;Neural networks;Performance evaluation;Recurrent neural networks;Signal processing algorithms;Signal resolution;System testing},
	doi={10.1109/IJCNN.2003.1224018},
	ISSN={1098-7576},
	month={July},
}

@article{steinbuch1961,
    added-at = {2008-02-26T11:58:58.000+0100},
    author = {Steinbuch, K.},
    citeulike-article-id = {2379314},
    description = {idsia},
    interhash = {5a92d070ec3a3266378ff839eecfcb01},
    intrahash = {0bd8f124747fa564fcbf21d8e3084f92},
    journal = {Kybernetic},
    keywords = {nn},
    pages = {36--45},
    priority = {2},
    timestamp = {2008-02-26T11:58:58.000+0100},
    title = {Die Lernmatrix},
    volume = 1,
    year = 1961
}
% biburl = {http://www.bibsonomy.org/bibtex/20bd8f124747fa564fcbf21d8e3084f92/schaul},

@article{steinbuch1963,
    author={Steinbuch, K. and Piske, U. A W},
    journal={Electronic Computers, IEEE Transactions on},
    title={Learning Matrices and Their Applications},
    year={1963},
    month={Dec},
    volume={EC-12},
    number={6},
    pages={846-862},
    keywords={Automatic control;Circuits;Filters;Frequency measurement;Machine learning;National electric code;Signal processing;Speech analysis;Speech processing;Speech recognition},
    doi={10.1109/PGEC.1963.263588},
    ISSN={0367-7508},
}

@misc{Tieleman2012,
	title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
	author={Tieleman, T. and Hinton, G.},
	howpublished={COURSERA: Neural Networks for Machine Learning},
	year={2012}
}

@article{Vogl1988,
	author="Vogl, T. P.
	and Mangis, J. K.
	and Rigler, A. K.
	and Zink, W. T.
	and Alkon, D. L.",
	title="Accelerating the convergence of the back-propagation method",
	journal="Biological Cybernetics",
	year="1988",
	volume="59",
	number="4",
	pages="257--263",
	abstract="The utility of the back-propagation method in establishing suitable weights in a distributed adaptive network has been demonstrated repeatedly. Unfortunately, in many applications, the number of iterations required before convergence can be large. Modifications to the back-propagation algorithm described by Rumelhart et al. (1986) can greatly accelerate convergence. The modifications consist of three changes:1) instead of updating the network weights after each pattern is presented to the network, the network is updated only after the entire repertoire of patterns to be learned has been presented to the network, at which time the algebraic sums of all the weight changes are applied:2) instead of keeping $\eta$, the ``learning rate'' (i.e., the multiplier on the step size) constant, it is varied dynamically so that the algorithm utilizes a near-optimum $\eta$, as determined by the local optimization topography; and3) the momentum factor $\alpha$ is set to zero when, as signified by a failure of a step to reduce the total error, the information inherent in prior steps is more likely to be misleading than beneficial. Only after the network takes a useful step, i.e., one that reduces the total error, does $\alpha$ again assume a non-zero value. Considering the selection of weights in neural nets as a problem in classical nonlinear optimization theory, the rationale for algorithms seeking only those weights that produce the globally minimum error is reviewed and rejected.",
	issn="1432-0770",
	doi="10.1007/BF00332914",
}

@phdthesis{Werbos1974,
	author = {Werbos, P. J.},
	interhash = {4165e2708a0468e89f8305f21ee2c711},
	intrahash = {b0644d7aa84be0df0f198d586d341843},
	keywords = {juergen},
	priority = {2},
	school = {Harvard University},
	timestamp = {2008-02-26T11:59:46.000+0100},
	title = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
	year = 1974
}

@article{Yin2007,
	author="Yin, Hong Xia
	and Du, Dong Lei",
	title="The Global Convergence of Self-Scaling BFGS Algorithmwith Nonmonotone Line Search forUnconstrained Nonconvex Optimization Problems",
	journal="Acta Mathematica Sinica, English Series",
	year="2007",
	volume="23",
	number="7",
	pages="1233--1240",
	abstract="The self-scaling quasi-Newton method solves an unconstrained optimization problem byscaling the Hessian approximation matrix before it is updated at each iteration to avoid the possiblelarge eigenvalues in the Hessian approximation matrices of the objective function. It has been provedin the literature that this method has the global and superlinear convergence when the objective functionis convex (or even uniformly convex). We propose to solve unconstrained nonconvex optimizationproblems by a self-scaling BFGS algorithm with nonmonotone linear search. Nonmonotone line searchhas been recognized in numerical practices as a competitive approach for solving large-scale nonlinearproblems. We consider two different nonmonotone line search forms and study the global convergenceof these nonmonotone self-scale BFGS algorithms. We prove that, under some weaker conditionthan that in the literature, both forms of the self-scaling BFGS algorithm are globally convergent forunconstrained nonconvex optimization problems.",
	issn="1439-7617",
	doi="10.1007/s10114-005-0837-5",
}
