@article{Amari1993,
	title = "Backpropagation and stochastic gradient descent method ",
	journal = "Neurocomputing ",
	volume = "5",
	number = "4–5",
	pages = "185 - 196",
	year = "1993",
	note = "",
	issn = "0925-2312",
	doi = "http://doi.org/10.1016/0925-2312(93)90006-O",
	url = "http://www.sciencedirect.com/science/article/pii/092523129390006O",
	author = "Shun-ichi Amari",
	keywords = "Stochastic descent",
	keywords = "generalized delta rule",
	keywords = "dynamics of learning",
	keywords = "pattern classification",
	keywords = "multilayer perceptron "
}


@article{anderson1968,
    author = {Anderson, J. A.},
    citeulike-article-id = {3641019},
    journal = {Kybernetik},
    pages = {113--119},
    posted-at = {2008-11-21 08:28:12},
    priority = {2},
    title = {{A memory storage model utilizing spatial correlation functions}},
    volume = {5},
    year = {1968}
}

@misc{anderson1970,
    title={Two models for memory organization using interacting traces},
    DOI={10.1016/0025-5564(70)90147-1},
    journal={Math. Biosci.},
    author={J. A. Anderson},
    year={1970}
}

@ARTICLE{Baldi1995,
	author={P. Baldi},
	journal={IEEE Transactions on Neural Networks},
	title={Gradient descent learning algorithm overview: a general dynamical systems perspective},
	year={1995},
	volume={6},
	number={1},
	pages={182-195},
	keywords={learning (artificial intelligence);neural nets;variational techniques;adjoint methods;backpropagation,;complexity;fixed point/trajectory learning;forward architecture;general dynamical systems perspective;gradient descent learning algorithm;neural networks;recurrent architecture;trajectory learning;variational calculus;Backpropagation algorithms;Biological neural networks;Biological systems;Calculus;Context modeling;Hebbian theory;Joining processes;Neurons;Organisms;Propulsion},
	doi={10.1109/72.363438},
	ISSN={1045-9227},
	month={Jan}
}

@ARTICLE{Bengio1994,
	author={Y. Bengio and P. Simard and P. Frasconi},
	journal={IEEE Transactions on Neural Networks},
	title={Learning long-term dependencies with gradient descent is difficult},
	year={1994},
	volume={5},
	number={2},
	pages={157-166},
	keywords={learning (artificial intelligence);numerical analysis;recurrent neural nets;efficient learning;gradient descent;input/output sequence mapping;long-term dependencies;prediction problems;production problems;recognition;recurrent neural network training;temporal contingencies;Computer networks;Cost function;Delay effects;Discrete transforms;Displays;Intelligent networks;Neural networks;Neurofeedback;Production;Recurrent neural networks},
	doi={10.1109/72.279181},
	ISSN={1045-9227},
	month={Mar},
}

@Book{Cruz2011,
    author = {Cruz, Pedro},
    title = {Inteligencia artificial con aplicaciones a la ingeniería},
    publisher = {Marcombo},
    year = {2011},
    address = {Barcelona},
    isbn = {978-8426717061}
}

@MASTERSTHESIS{Fritsch1996,
	Title	= {Modular Neural Networks for Speech Recognition},
	Author	= {Jürgen Fritsch},
	School	= {KIT},
	Year	= {1996},
	Month	= {July 31},
	Type	= {Masters Thesis},
	File	= {:Fritsch1996b.pdf:PDF},
	Link	= {http://isl.anthropomatik.kit.edu/cmu-kit/english/4894.php},
}

Newell1969b
@book{hebb2002,
    title={The Organization of Behavior: A Neuropsychological Theory},
    author={Hebb, D.O.},
    isbn={9781410612403},
    series={E-Books von NetLibrary},
    year={2002},
    publisher={Taylor \& Francis}
}

@article{hopfield1982,
    abstract = {{Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}},
    author = {Hopfield, J. J.},
    citeulike-article-id = {878138},
    citeulike-linkout-0 = {http://www.pnas.org/content/79/8/2554.abstract},
    citeulike-linkout-1 = {http://www.pnas.org/content/79/8/2554.full.pdf},
    citeulike-linkout-2 = {http://www.pnas.org/cgi/content/abstract/79/8/2554},
    citeulike-linkout-3 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=6953413},
    day = {1},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    keywords = {emergence, neural\_computation},
    month = apr,
    number = {8},
    pages = {2554--2558},
    pmcid = {PMC346238},
    pmid = {6953413},
    posted-at = {2009-10-06 16:50:26},
    priority = {4},
    publisher = {National Academy of Sciences},
    title = {{Neural networks and physical systems with emergent collective computational abilities.}},
    volume = {79},
    year = {1982}
}

@MASTERSTHESIS {Hochreiter1991,
    author = "Sepp Hochreiter",
    title  = "Untersuchungen zu dynamischen neuronalen Netzen",
    school = "Institut f. Informatik, Technische Univ. Munich",
    year   = "1991",
    type   = "Diploma thesis"
}

@article{Hochreiter1997a,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    journal = {Neural Comput.},
    issue_date = {November 15, 1997},
    volume = {9},
    number = {8},
    month = nov,
    year = {1997},
    issn = {0899-7667},
    pages = {1735--1780},
    numpages = {46},
    url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    acmid = {1246450},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
}

@article{Hochreiter1998b,
 author = {Hochreiter, Sepp},
 title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
 journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
 issue_date = {April 1998},
 volume = {6},
 number = {2},
 month = apr,
 year = {1998},
 issn = {0218-4885},
 pages = {107--116},
 numpages = {10},
 url = {http://dx.doi.org/10.1142/S0218488598000094},
 doi = {10.1142/S0218488598000094},
 acmid = {355233},
 publisher = {World Scientific Publishing Co., Inc.},
 address = {River Edge, NJ, USA},
 keywords = {long short-term memory, long-term dependencies, recurrent neural nets, vanishing gradient},
}

@ARTICLE{Huang2005,
	author={Guang-Bin Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation},
	year={2005},
	volume={16},
	number={1},
	pages={57-67},
	keywords={function approximation;learning (artificial intelligence);radial basis function networks;function approximation;growing RBF neural network;pruning RBF neural network;radial basis function networks;sequential learning algorithm;Approximation algorithms;Backpropagation algorithms;Function approximation;Joining processes;Learning systems;Neural networks;Neurons;Radial basis function networks;Sampling methods;Training data;Growing;neuron's significance;pruning;radial basis networks;sequential learning;Algorithms;Artificial Intelligence;Cluster Analysis;Computing Methodologies;Neural Networks (Computer);Numerical Analysis, Computer-Assisted},
	doi={10.1109/TNN.2004.836241},
	ISSN={1045-9227},
	month={Jan},
}

@misc{Keras2015,
	title={Keras},
	author={Chollet, Fran\c{c}ois},
	year={2015},
	publisher={GitHub},
	howpublished={\url{https://github.com/fchollet/keras}},
}

@article{kohonen1972,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={Correlation Matrix Memories},
    year={1972},
    month={April},
    volume={C-21},
    number={4},
    pages={353-359},
    keywords={Associative memory;Biological information theory;Biomedical optical imaging;Encoding;Holographic optical components;Holography;Mathematical model;Sampling methods;Signal analysis;Stochastic processes;Associative memory;associative net;associative recall;correlation matrix memory;nonholographic associative memory;pattern recognition},
    doi={10.1109/TC.1972.5008975},
    ISSN={0018-9340},
}

@article{kohonen1974,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={An Adaptive Associative Memory Principle},
    year={1974},
    month={April},
    volume={C-23},
    number={4},
    pages={444-445},
    keywords={Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.;Associative memory;Circuits;Distributed computing;Gradient methods;Information systems;Learning systems;Pattern recognition;Physics computing;Quantum computing;Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.},
    doi={10.1109/T-C.1974.223960},
    ISSN={0018-9340},
}

@ARTICLE{Lian2006,
	author={N. y. Liang and G. b. Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks},
	year={2006},
	volume={17},
	number={6},
	pages={1411-1423},
	keywords={learning (artificial intelligence);radial basis function networks;additive hidden mode;batch learning;bounded nonconstant piecewise continuous functions;online sequential extreme learning machine;radial basis function hidden mode;single hidden layer feedforward networks;Convergence;Industrial training;Machine learning;Neural networks;Radial basis function networks;Radio access networks;Resource management;Spine;Stochastic processes;Training data;Extreme learning machine (ELM);GGAP-RBF;growing and pruning RBF network (GAP-RBF);minimal resource allocation network (MRAN);online sequential ELM (OS-ELM);resource allocation network (RAN);resource allocation network via extended kalman filter (RANEKF);stochastic gradient descent back-propagation (SGBP);Algorithms;Information Storage and Retrieval;Information Theory;Neural Networks (Computer);Online Systems;Pattern Recognition, Automated;Signal Processing, Computer-Assisted},
	doi={10.1109/TNN.2006.880583},
	ISSN={1045-9227},
	month={Nov},
}

@Article{McCulloch1943,
	author="McCulloch, Warren S. and Pitts, Walter",
	title="A logical calculus of the ideas immanent in nervous activity",
	journal="The bulletin of mathematical biophysics",
	year="1943",
	volume="5",
	number="4",
	pages="115--133",
	abstract="Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
	issn="1522-9602",
	doi="10.1007/BF02478259",
}

@article{mcculloch_pitts1943,
    year={1943},
    issn={0007-4985},
    journal={The bulletin of mathematical biophysics},
    volume={5},
    number={4},
    doi={10.1007/BF02478259},
    title={A logical calculus of the ideas immanent in nervous activity},
    publisher={Kluwer Academic Publishers},
    author={McCulloch, WarrenS. and Pitts, Walter},
    pages={115-133},
    language={English}
}

@article {Minsky1969,
	author = {Newell, Allen},
	title = {Perceptrons: An Introduction to Computational Geometry. Marvin Minsky and Seymour Papert. M.I.T. Press, Cambridge, Mass., 1969. vi + 258 pp., illus. Cloth, $12; paper, $4.95},
	volume = {165},
	number = {3895},
	pages = {780--782},
	year = {1969},
	doi = {10.1126/science.165.3895.780},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/165/3895/780},
	eprint = {http://science.sciencemag.org/content/165/3895/780.full.pdf},
	journal = {Science}
}

@book{Minsky1969b,
	title = {Perceptrons: An Introduction to Computational Geometry},
	author = {Marvin Minsky, Seymour A. Papert},
	publisher = {The MIT Press},
	isbn = {0262631113,9780262631112},
	year = {1987},
	series = {},
	edition = {Expanded},
	volume = {},
}

@inproceedings{Morse2016,
	author = {Morse, Gregory and Stanley, Kenneth O.},
	title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	series = {GECCO '16},
	year = {2016},
	isbn = {978-1-4503-4206-3},
	location = {Denver, Colorado, USA},
	pages = {477--484},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/2908812.2908916},
	doi = {10.1145/2908812.2908916},
	acmid = {2908916},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
}

@TechReport{rosenblatt1957,
    author= {F. Rosenblatt},
    title= {The perceptron---a perceiving and recognizing automaton},
    type= {Report},
    number= {85-460-1},
    institution= {Cornell Aeronautical Laboratory},
    year= 1957,
    comment= PRNNref,
}

@book{rosenblatt1958,
    title={The perceptron: a theory of statistical separability in cognitive systems (Project Para)},
    author={Rosenblatt, F. and Cornell Aeronautical Laboratory},
    series={Report},
    year={1958},
    publisher={Cornell Aeronautical Laboratory}
}

@article{Rumelhart1986,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	journal={Nature},
	volume={323},
	pages={533--536},
	year={1986}
}

@Book{Sampieri2006,
    author = {Sampieri, Roberto},
    title = {Metodolog\'{i}a de la investigaci\'{o}n},
    publisher = {McGraw Hill},
    year = {2006},
    address = {M\'{e}xico},
    isbn = {970-10-5753-8}
}

@article{steinbuch1961,
    added-at = {2008-02-26T11:58:58.000+0100},
    author = {Steinbuch, K.},
    citeulike-article-id = {2379314},
    description = {idsia},
    interhash = {5a92d070ec3a3266378ff839eecfcb01},
    intrahash = {0bd8f124747fa564fcbf21d8e3084f92},
    journal = {Kybernetic},
    keywords = {nn},
    pages = {36--45},
    priority = {2},
    timestamp = {2008-02-26T11:58:58.000+0100},
    title = {Die Lernmatrix},
    volume = 1,
    year = 1961
}
% biburl = {http://www.bibsonomy.org/bibtex/20bd8f124747fa564fcbf21d8e3084f92/schaul},

@article{steinbuch1963,
    author={Steinbuch, K. and Piske, U. A W},
    journal={Electronic Computers, IEEE Transactions on},
    title={Learning Matrices and Their Applications},
    year={1963},
    month={Dec},
    volume={EC-12},
    number={6},
    pages={846-862},
    keywords={Automatic control;Circuits;Filters;Frequency measurement;Machine learning;National electric code;Signal processing;Speech analysis;Speech processing;Speech recognition},
    doi={10.1109/PGEC.1963.263588},
    ISSN={0367-7508},
}
