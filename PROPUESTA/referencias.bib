@article{Amari1993,
	title = "Backpropagation and stochastic gradient descent method ",
	journal = "Neurocomputing ",
	volume = "5",
	number = "4–5",
	pages = "185 - 196",
	year = "1993",
	note = "",
	issn = "0925-2312",
	doi = "http://doi.org/10.1016/0925-2312(93)90006-O",
	url = "http://www.sciencedirect.com/science/article/pii/092523129390006O",
	author = "Shun-ichi Amari",
	keywords = "Stochastic descent",
	keywords = "generalized delta rule",
	keywords = "dynamics of learning",
	keywords = "pattern classification",
	keywords = "multilayer perceptron "
}


@article{anderson1968,
    author = {Anderson, J. A.},
    citeulike-article-id = {3641019},
    journal = {Kybernetik},
    pages = {113--119},
    posted-at = {2008-11-21 08:28:12},
    priority = {2},
    title = {{A memory storage model utilizing spatial correlation functions}},
    volume = {5},
    year = {1968}
}

@misc{anderson1970,
    title={Two models for memory organization using interacting traces},
    DOI={10.1016/0025-5564(70)90147-1},
    journal={Math. Biosci.},
    author={J. A. Anderson},
    year={1970}
}

@ARTICLE{Baldi1995,
	author={P. Baldi},
	journal={IEEE Transactions on Neural Networks},
	title={Gradient descent learning algorithm overview: a general dynamical systems perspective},
	year={1995},
	volume={6},
	number={1},
	pages={182-195},
	keywords={learning (artificial intelligence);neural nets;variational techniques;adjoint methods;backpropagation,;complexity;fixed point/trajectory learning;forward architecture;general dynamical systems perspective;gradient descent learning algorithm;neural networks;recurrent architecture;trajectory learning;variational calculus;Backpropagation algorithms;Biological neural networks;Biological systems;Calculus;Context modeling;Hebbian theory;Joining processes;Neurons;Organisms;Propulsion},
	doi={10.1109/72.363438},
	ISSN={1045-9227},
	month={Jan}
}

@inproceedings{Bengio1993,
  author    = {Yoshua Bengio and
               Paolo Frasconi},
  title     = {Credit Assignment through Time: Alternatives to Backpropagation},
  booktitle = {Advances in Neural Information Processing Systems 6, [7th {NIPS} Conference,
               Denver, Colorado, USA, 1993]},
  pages     = {75--82},
  year      = {1993},
  url       = {http://papers.nips.cc/paper/724-credit-assignment-through-time-alternatives-to-backpropagation},
  timestamp = {Thu, 11 Dec 2014 17:34:08 +0100},
  }

@article{Bengio1994,
	author={Y. Bengio and P. Simard and P. Frasconi},
	journal={IEEE Transactions on Neural Networks},
	title={Learning long-term dependencies with gradient descent is difficult},
	year={1994},
	volume={5},
	number={2},
	pages={157-166},
	keywords={learning (artificial intelligence);numerical analysis;recurrent neural nets;efficient learning;gradient descent;input/output sequence mapping;long-term dependencies;prediction problems;production problems;recognition;recurrent neural network training;temporal contingencies;Computer networks;Cost function;Delay effects;Discrete transforms;Displays;Intelligent networks;Neural networks;Neurofeedback;Production;Recurrent neural networks},
	doi={10.1109/72.279181},
	ISSN={1045-9227},
	month={Mar},
}

@Book{Cruz2011,
    author = {Cruz, Pedro},
    title = {Inteligencia artificial con aplicaciones a la ingeniería},
    publisher = {Marcombo},
    year = {2011},
    address = {Barcelona},
    isbn = {978-8426717061}
}

@article{Elman1990,
	title = "Finding structure in time ",
	journal = "Cognitive Science ",
	volume = "14",
	number = "2",
	pages = "179 - 211",
	year = "1990",
	note = "",
	issn = "0364-0213",
	doi = "https://doi.org/10.1016/0364-0213(90)90002-E",
	url = "http://www.sciencedirect.com/science/article/pii/036402139090002E",
	author = "Jeffrey L. Elman",
	abstract = "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
}
@MASTERSTHESIS{Fritsch1996,
	Title	= {Modular Neural Networks for Speech Recognition},
	Author	= {Jürgen Fritsch},
	School	= {KIT},
	Year	= {1996},
	Month	= {July 31},
	Type	= {Masters Thesis},
	File	= {:Fritsch1996b.pdf:PDF},
	Link	= {http://isl.anthropomatik.kit.edu/cmu-kit/english/4894.php},
}

Newell1969b
@book{hebb2002,
    title={The Organization of Behavior: A Neuropsychological Theory},
    author={Hebb, D.O.},
    isbn={9781410612403},
    series={E-Books von NetLibrary},
    year={2002},
    publisher={Taylor \& Francis}
}

@article{hopfield1982,
    abstract = {{Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}},
    author = {Hopfield, J. J.},
    citeulike-article-id = {878138},
    citeulike-linkout-0 = {http://www.pnas.org/content/79/8/2554.abstract},
    citeulike-linkout-1 = {http://www.pnas.org/content/79/8/2554.full.pdf},
    citeulike-linkout-2 = {http://www.pnas.org/cgi/content/abstract/79/8/2554},
    citeulike-linkout-3 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=6953413},
    day = {1},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    keywords = {emergence, neural\_computation},
    month = apr,
    number = {8},
    pages = {2554--2558},
    pmcid = {PMC346238},
    pmid = {6953413},
    posted-at = {2009-10-06 16:50:26},
    priority = {4},
    publisher = {National Academy of Sciences},
    title = {{Neural networks and physical systems with emergent collective computational abilities.}},
    volume = {79},
    year = {1982}
}

@MASTERSTHESIS {Hochreiter1991,
    author = "Sepp Hochreiter",
    title  = "Untersuchungen zu dynamischen neuronalen Netzen",
    school = "Institut f. Informatik, Technische Univ. Munich",
    year   = "1991",
    type   = "Diploma thesis"
}

@article{Hochreiter1997a,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    journal = {Neural Comput.},
    issue_date = {November 15, 1997},
    volume = {9},
    number = {8},
    month = nov,
    year = {1997},
    issn = {0899-7667},
    pages = {1735--1780},
    numpages = {46},
    url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    acmid = {1246450},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
}

@article{Hochreiter1998b,
 author = {Hochreiter, Sepp},
 title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
 journal = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
 issue_date = {April 1998},
 volume = {6},
 number = {2},
 month = apr,
 year = {1998},
 issn = {0218-4885},
 pages = {107--116},
 numpages = {10},
 url = {http://dx.doi.org/10.1142/S0218488598000094},
 doi = {10.1142/S0218488598000094},
 acmid = {355233},
 publisher = {World Scientific Publishing Co., Inc.},
 address = {River Edge, NJ, USA},
 keywords = {long short-term memory, long-term dependencies, recurrent neural nets, vanishing gradient},
}

@ARTICLE{Huang2005,
	author={Guang-Bin Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation},
	year={2005},
	volume={16},
	number={1},
	pages={57-67},
	keywords={function approximation;learning (artificial intelligence);radial basis function networks;function approximation;growing RBF neural network;pruning RBF neural network;radial basis function networks;sequential learning algorithm;Approximation algorithms;Backpropagation algorithms;Function approximation;Joining processes;Learning systems;Neural networks;Neurons;Radial basis function networks;Sampling methods;Training data;Growing;neuron's significance;pruning;radial basis networks;sequential learning;Algorithms;Artificial Intelligence;Cluster Analysis;Computing Methodologies;Neural Networks (Computer);Numerical Analysis, Computer-Assisted},
	doi={10.1109/TNN.2004.836241},
	ISSN={1045-9227},
	month={Jan},
}

@misc{Keras2015,
	title={Keras},
	author={Chollet, Fran\c{c}ois},
	year={2015},
	publisher={GitHub},
	howpublished={\url{https://github.com/fchollet/keras}},
}

@article{kohonen1972,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={Correlation Matrix Memories},
    year={1972},
    month={April},
    volume={C-21},
    number={4},
    pages={353-359},
    keywords={Associative memory;Biological information theory;Biomedical optical imaging;Encoding;Holographic optical components;Holography;Mathematical model;Sampling methods;Signal analysis;Stochastic processes;Associative memory;associative net;associative recall;correlation matrix memory;nonholographic associative memory;pattern recognition},
    doi={10.1109/TC.1972.5008975},
    ISSN={0018-9340},
}

@article{kohonen1974,
    author={Kohonen, T.},
    journal={Computers, IEEE Transactions on},
    title={An Adaptive Associative Memory Principle},
    year={1974},
    month={April},
    volume={C-23},
    number={4},
    pages={444-445},
    keywords={Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.;Associative memory;Circuits;Distributed computing;Gradient methods;Information systems;Learning systems;Pattern recognition;Physics computing;Quantum computing;Adaptive system, analog associative memory, associative recall, correlation matrix memory, distributed memory, learning system, pattern recognition, pseudoinverse matrix.},
    doi={10.1109/T-C.1974.223960},
    ISSN={0018-9340},
}

@ARTICLE{Lian2006,
	author={N. y. Liang and G. b. Huang and P. Saratchandran and N. Sundararajan},
	journal={IEEE Transactions on Neural Networks},
	title={A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks},
	year={2006},
	volume={17},
	number={6},
	pages={1411-1423},
	keywords={learning (artificial intelligence);radial basis function networks;additive hidden mode;batch learning;bounded nonconstant piecewise continuous functions;online sequential extreme learning machine;radial basis function hidden mode;single hidden layer feedforward networks;Convergence;Industrial training;Machine learning;Neural networks;Radial basis function networks;Radio access networks;Resource management;Spine;Stochastic processes;Training data;Extreme learning machine (ELM);GGAP-RBF;growing and pruning RBF network (GAP-RBF);minimal resource allocation network (MRAN);online sequential ELM (OS-ELM);resource allocation network (RAN);resource allocation network via extended kalman filter (RANEKF);stochastic gradient descent back-propagation (SGBP);Algorithms;Information Storage and Retrieval;Information Theory;Neural Networks (Computer);Online Systems;Pattern Recognition, Automated;Signal Processing, Computer-Assisted},
	doi={10.1109/TNN.2006.880583},
	ISSN={1045-9227},
	month={Nov},
}

@article{Lipton2015,
  author    = {Zachary Chase Lipton},
  title     = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1506.00019},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.00019},
  timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Lipton15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{McCulloch1943,
	author="McCulloch, Warren S. and Pitts, Walter",
	title="A logical calculus of the ideas immanent in nervous activity",
	journal="The bulletin of mathematical biophysics",
	year="1943",
	volume="5",
	number="4",
	pages="115--133",
	abstract="Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
	issn="1522-9602",
	doi="10.1007/BF02478259",
}

@article{mcculloch_pitts1943,
    year={1943},
    issn={0007-4985},
    journal={The bulletin of mathematical biophysics},
    volume={5},
    number={4},
    doi={10.1007/BF02478259},
    title={A logical calculus of the ideas immanent in nervous activity},
    publisher={Kluwer Academic Publishers},
    author={McCulloch, WarrenS. and Pitts, Walter},
    pages={115-133},
    language={English}
}

@article {Minsky1969,
	author = {Newell, Allen},
	title = {Perceptrons: An Introduction to Computational Geometry. Marvin Minsky and Seymour Papert. M.I.T. Press, Cambridge, Mass., 1969. vi + 258 pp., illus. Cloth, $12; paper, $4.95},
	volume = {165},
	number = {3895},
	pages = {780--782},
	year = {1969},
	doi = {10.1126/science.165.3895.780},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/165/3895/780},
	eprint = {http://science.sciencemag.org/content/165/3895/780.full.pdf},
	journal = {Science}
}

@book{Minsky1969b,
	title = {Perceptrons: An Introduction to Computational Geometry},
	author = {Marvin Minsky, Seymour A. Papert},
	publisher = {The MIT Press},
	isbn = {0262631113,9780262631112},
	year = {1987},
	series = {},
	edition = {Expanded},
	volume = {},
}

@inproceedings{Morse2016,
	author = {Morse, Gregory and Stanley, Kenneth O.},
	title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	series = {GECCO '16},
	year = {2016},
	isbn = {978-1-4503-4206-3},
	location = {Denver, Colorado, USA},
	pages = {477--484},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/2908812.2908916},
	doi = {10.1145/2908812.2908916},
	acmid = {2908916},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
}

@article{Pearlmutter1989,
author={B. A. Pearlmutter},
journal={Neural Computation},
title={Learning State Space Trajectories in Recurrent Neural Networks},
year={1989},
volume={1},
number={2},
pages={263-269},
doi={10.1162/neco.1989.1.2.263},
ISSN={0899-7667},
month={June},
}

@ARTICLE{Pearlmutter1995,
author={B. A. Pearlmutter},
journal={IEEE Transactions on Neural Networks},
title={Gradient calculations for dynamic recurrent neural networks: a survey},
year={1995},
volume={6},
number={5},
pages={1212-1228},
keywords={Boltzmann machines;backpropagation;recurrent neural nets;Elman's history cutoff;Jordan's output feedback architecture;backpropagation through time;computational complexity;deterministic Boltzmann machines;dynamic recurrent neural networks;fixed point learning algorithms;forward propagation;gradient calculations;learning speed;nonfixed point algorithms;recurrent backpropagation;temporally continuous neural networks;Backpropagation algorithms;Clocks;Computational complexity;Computational modeling;Equations;History;Machine learning;Neural networks;Output feedback;Recurrent neural networks},
doi={10.1109/72.410363},
ISSN={1045-9227},
month={Sep},
}

@article{Puskorius1994,
	author={G. V. Puskorius and L. A. Feldkamp},
	journal={IEEE Transactions on Neural Networks},
	title={Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks},
	year={1994},
	volume={5},
	number={2},
	pages={279-297},
	keywords={Kalman filters;filtering and prediction theory;nonlinear control systems;nonlinear dynamical systems;recurrent neural nets;Kalman filter trained recurrent networks;automotive subsystem;bioreactor benchmark problems;cart-pole;decoupled extended Kalman filter;engine idle speed control;homogeneous architecture;neurocontrol;nonlinear dynamical systems;parameter-based extended Kalman filter algorithms;state-space controllers;state-space observers;Automotive engineering;Bioreactors;Control systems;Engines;Neural networks;Noise measurement;Nonlinear control systems;Nonlinear dynamical systems;Recurrent neural networks;Velocity control},
	doi={10.1109/72.279191},
	ISSN={1045-9227},
	month={Mar},
}

@article{Rios2013,
	author="Rios, Luis Miguel
	and Sahinidis, Nikolaos V.",
	title="Derivative-free optimization: a review of algorithms and comparison of software implementations",
	journal="Journal of Global Optimization",
	year="2013",
	volume="56",
	number="3",
	pages="1247--1293",
	abstract="This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.",
	issn="1573-2916",
	doi="10.1007/s10898-012-9951-y",
	url="http://dx.doi.org/10.1007/s10898-012-9951-y"
}


@TechReport{rosenblatt1957,
    author= {F. Rosenblatt},
    title= {The perceptron---a perceiving and recognizing automaton},
    type= {Report},
    number= {85-460-1},
    institution= {Cornell Aeronautical Laboratory},
    year= 1957,
    comment= PRNNref,
}

@book{rosenblatt1958,
    title={The perceptron: a theory of statistical separability in cognitive systems (Project Para)},
    author={Rosenblatt, F. and Cornell Aeronautical Laboratory},
    series={Report},
    year={1958},
    publisher={Cornell Aeronautical Laboratory}
}

@article{Rumelhart1986,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	journal={Nature},
	volume={323},
	pages={533--536},
	year={1986}
}

@Book{Sampieri2006,
    author = {Sampieri, Roberto},
    title = {Metodolog\'{i}a de la investigaci\'{o}n},
    publisher = {McGraw Hill},
    year = {2006},
    address = {M\'{e}xico},
    isbn = {970-10-5753-8}
}

@article{Schmidhuber1992a,
	author = {Schmidhuber, J\"{u}rgen},
	title = {Learning Complex, Extended Sequences Using the Principle of History Compression},
	journal = {Neural Comput.},
	issue_date = {March 1992},
	volume = {4},
	number = {2},
	month = {mar},
	year = {1992},
	issn = {0899-7667},
	pages = {234--242},
	numpages = {9},
	url = {http://dx.doi.org/10.1162/neco.1992.4.2.234},
	doi = {10.1162/neco.1992.4.2.234},
	acmid = {148076},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@article{Schmidhuber1992b,
	title={A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks},
	author={J\''{u}rgen Schmidhuber},
	journal={Neural Computation},
	year={1992},
	volume={4},
	pages={243-248},
}

@TECHREPORT{Schmidhuber1996,
    author = {J{\"u}rgen Schmidhuber and Sepp Hochreiter},
    title = {Guessing Can Outperform Many Long Time Lag Algorithms},
    institution = {},
    year = {1996}
}

@INPROCEEDINGS{Squartini2003a,
	author={S. Squartini and A. Hussain and F. Piazza},
	booktitle={Circuits and Systems, 2003. ISCAS '03. Proceedings of the 2003 International Symposium on},
	title={Preprocessing based solution for the vanishing gradient problem in recurrent neural networks},
	year={2003},
	volume={5},
	pages={V-713-V-716 vol.5},
	keywords={discrete wavelet transforms;gradient methods;learning (artificial intelligence);recurrent neural nets;time series;DWT;RMN;RNN learning;RNN signal preprocessing;frequency scales;latching problem;learning techniques;network architecture;partial result combination;recurrent multiscale network;recurrent neural networks;short/long term information separation;time scales;time series prediction;vanishing gradient problem;wavelet decomposition;Computer architecture;Computer networks;Delay effects;Digital signal processing;Electronic mail;Information analysis;Intelligent networks;Neural networks;Recurrent neural networks;Signal processing algorithms},
	doi={10.1109/ISCAS.2003.1206412},
	month={May},
}

@INPROCEEDINGS{Squartini2003b,
	author={S. Squartini and A. Hussain and F. Piazza},
	booktitle={Proceedings of the International Joint Conference on Neural Networks, 2003.},
	title={Attempting to reduce the vanishing gradient effect through a novel recurrent multiscale architecture},
	year={2003},
	volume={4},
	pages={2819-2824 vol.4},
	keywords={learning (artificial intelligence);neural net architecture;recurrent neural nets;signal processing;time series;wavelet transforms;adaptive nonlinear structure;discrete wavelet decomposition;latching problem;long term dependencies detection;long term information;recurrent multiscale architecture;short term information;time series prediction;vanishing gradient effect;Computer architecture;Computer hacking;Delay effects;Discrete wavelet transforms;Neural networks;Performance evaluation;Recurrent neural networks;Signal processing algorithms;Signal resolution;System testing},
	doi={10.1109/IJCNN.2003.1224018},
	ISSN={1098-7576},
	month={July},
}

@article{steinbuch1961,
    added-at = {2008-02-26T11:58:58.000+0100},
    author = {Steinbuch, K.},
    citeulike-article-id = {2379314},
    description = {idsia},
    interhash = {5a92d070ec3a3266378ff839eecfcb01},
    intrahash = {0bd8f124747fa564fcbf21d8e3084f92},
    journal = {Kybernetic},
    keywords = {nn},
    pages = {36--45},
    priority = {2},
    timestamp = {2008-02-26T11:58:58.000+0100},
    title = {Die Lernmatrix},
    volume = 1,
    year = 1961
}
% biburl = {http://www.bibsonomy.org/bibtex/20bd8f124747fa564fcbf21d8e3084f92/schaul},

@article{steinbuch1963,
    author={Steinbuch, K. and Piske, U. A W},
    journal={Electronic Computers, IEEE Transactions on},
    title={Learning Matrices and Their Applications},
    year={1963},
    month={Dec},
    volume={EC-12},
    number={6},
    pages={846-862},
    keywords={Automatic control;Circuits;Filters;Frequency measurement;Machine learning;National electric code;Signal processing;Speech analysis;Speech processing;Speech recognition},
    doi={10.1109/PGEC.1963.263588},
    ISSN={0367-7508},
}
