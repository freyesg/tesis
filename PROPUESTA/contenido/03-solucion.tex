\section{Descripción de la solución}
En la presente sección se describe el estado del arte y las características de la solución. Se explicara cual es el propósito de la solución, y posteriormente los alcances y limitaciones establecidas.

\subsection{Estado del arte}
La más frecuente técnica para inducir cambios rápidos y significativos en la \pam\, ha sido la deflación repentina de los manguitos sobre el muslo \citep{Aaslid1989, Aaslid1991,Lagi1994, Newell1994, Strebel1995, Tiecks1995a}. Con este enfoque, se colocan los manguitos alrededor de ambos muslos y se  genera una presión de \mmhg{20-40} por encima de la presión sistólica durante 2 minutos. Sin embargo, con una autorregulación normal la \cbfv\, vuelve y alcanza su estado basal antes que la \pam. \cite{Aaslid1989} demuestran que la velocidad de recuperación se ve afectada de manera significativa por los niveles de la \pacoo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% TRANSFER FUNCTION ANALYSIS %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
La evaluación de la \ca\, mediante el análisis de la función de transferencia está basado en minimizar, en la \ca, el efecto de la oscilación espontánea sobre la \cbfv. El método ha sido extensamente utilizado, por ejemplo, en la investivación del control cardiocasvular, arrítmia sinusal respiratoria y autoregulación renal \citep{Saul1989, Saul1991, Holstein1991}. El análisis espectral, al igual que la transformada rápida de Fourier, transforma la serie en el tiempo de la \bp\, y la \cbfv\, al dominio de la frecuencia. Entonces, la función de transferencia entre las dos señales se calcula como: $$ H(f) = \frac{S_{xy}(f)}{S_{xx}(f)} $$ donde $S_{xx}(f)$ es el autoespectro entre la señal de entrada, \bp, y la de señal de salida, \cbfv \citep{Ainslie2008}. Con la función de potencia relativa asociada (ganancia) y al tiempo (fase) puede ser descrito usando la parte real $H_{R}(f)$ y la parte imaginaria $H_{I}(f)$ de la función de transferencia compleja
\begin{eqnarray}
    \mbox{ganancia : } |H(f)| &=& \sqrt{|H_{R}(f)|^2 + |H_{I}(f)|^2}\\
    \mbox{fase : } \Phi(f) &=& \tan^{-1}\left(\frac{|H_{R}(f)|}{|H_{I}(f)|}\right)
\end{eqnarray}

Una estimación de la fiabilidad de la relación entre las dos señales se puede encontrar como la coherencia cuadrado:
\begin{eqnarray}
    \mbox{coherencia : } MSC(f) &=& \frac{|S_{xy}(f)|^2}{S_{xx}(f)S_{yy}(f)}
\end{eqnarray}
donde $S_{yy}(f)$ es el autoespectro del cambio en la \cbfv.



Como una representación de la relación lineal entre la fluctuación de la presión arterial y el flujo sanguíneo cerebral, la coherencia también se utiliza como una medida de \ca. Una coherencia cercana a cero indica que no hay relación entre la presión arterial y el flujo sanguíneo cerebral, mientras que una coherencia cercana a la unidad sugiere una relación lineal indicando problemas en la \ca.

En los primeros estudios de la \ca\, usando la función de transferencia y la fluctuación espontánea de la \bp, se obtuvieron las estimaciones de la respuesta de la frecuencia de amplitud (ganancia) y la coherencia, pero no la respuesta de frecuencia de fase \citep{Giller1990}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% VOLTERRA %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
La presencia de no linealidades en la hemodinámica cerebral se ha sugerido en numerosos estudios \citep{Mitsis2002, Zhang1998, Mitsis2004a, Giller2003}. Por lo tanto, un modelo general de Volterra de dos entradas permitiría describir cuantitativamente los efectos dinámicos de la \abp\, espontánea media y la presión de \etcoo.
%Se ha descrito la serie de expansión de Volterra como una solución para un sistema dinámico no lineal, y puede ser descrito como
%\begin{equation}
%    V_M(t) = k_0 + \sum_{m = 0}^{L - 1} k_1(m)P(t - 1) + \sum_{m_1 = 0}^{L - 1} \sum_{m_2 = 0}^{L - 1} k_2(m_1, m_2)\cdot P(t - m_1)\cdot P(t - m_2) + \cdots\label{eq:volterra}
%\end{equation}

%Donde $P(t)$ es la entrada de \abp\, y $V_M(t)$ es el modelo de velocidad de salida. $k_1(t)$ y $k_2(t)$ son los núcleos de primer y segundo orden. Y $L$ es el número de intervalos de tiempo de retardo utilizados para representar la duración de los núcleos. %Los núcleos de primer y segundo orden pueden obtenerse como sigue
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% RECURRENT NEURAL NETWORK %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cite{Panerai2004} propone las redes neuronales recurrentes con retraso ({\em Time Lagged Recurrent Network}, TLRN) para modelar la relación dinámica entre la \pam\, y la \cbfv. Esta arquitectura particular, representa una visión general para explorar la aplicabilidad de una RNA para el modelamiento de la \cbf, debido a la flexibilidad de utilizar diferentes elementos de proceso ({\em Processing elements}, PE), que contiene información que permite modelar comportamiento dinámico. Memorias a corto plazo de Gamma y Laguerre han sido sugueridos como bloques de construcción para este fin. %La memoria Laguerre tiene un solo parámetro de $\mu$, que es posible calcularla mediante la forma recursiva
%\begin{eqnarray}
%    L_{k}(z, \mu) &=& G_{k - 1}(z)\frac{\sqrt{1 - (1 - \mu)^{2}}}{1 - (1 - \mu)z^{-1}}\\
%    G_{k - 1} &=& \prod_{i = 1}^{k - 1} \left(\frac{z^{-1}(1 - \mu)}{1 - (1 - \mu)z^{-1}}\right)i
%\end{eqnarray}

La memoria de Laguerre es una combinación en cascada de filtros de paso bajo y paso todo. Las señales basales se obtienen de la convolución de la entrada del filtro de pasa bajo con un conjunto ortogonal de la función de paso todo, por lo cual la correlación de la base es menor que la de una memoria Gamma y la velocidad de adaptación aumenta. Cada filtro de paso todo coloca un cero en el círculo unitario, lo que indica el polo de la etapa anterior. Como resultado, la memoria de Laguerre no atenúa las señales de base.

La arquitectura propuesta por \cite{Panerai2004} posee una memoria Laguerre con tres entradas basales de \abp, una capa oculta con \pe\, no lineales conectados directamente a la \abp\, actual de base, las bases restantes de la memoria Laguerre, y los ciclos de retroalimentación de la memoria \cbfv. Se establece un umbral \pe\, para ajustar la salida. Finalmente, una capa de salida para la \cbfv\, contiene otra memoria Laguerre con tres bases con ciclos de retroalimentación de retardo a la capa oculta \pe.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% SUPPORT VECTOR MACHINE %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cite{Chacon2011} presenta un modelo basado en $v-SVM$, introducido por \cite{Scholkopf2000}. El algoritmo se basa en el aprendizaje estadístico que funciona como un tubo de radio $\varepsilon$ que encierra a los datos. La frontera de decisión que determina el radio $\varepsilon$ del tubo se obtiene mediante el uso de un subconjunto de ejemplos de entrenamiento llamado Vector de Soporte ({\em Support vector}, \sv). Si $\vec{x}$ representa al vector de datos de entrada, el valor de salida $f(\vec{x})$ está dado por la regresión \svm\, utilizando un vector de pesos $\vec{w}$
%\begin{equation}
%    f(\vec{x}) = (\vec{w}\cdot\vec{x}) + b \,\,\,\,\,\, \vec{w}, \vec{x} \in \mathbb{R}^{N}, b \in \mathbb{R}\label{eq:svm}
%\end{equation}
%donde $b$ es una constante obtenida desde $\vec{w}$.

Una característica importante de las \svm\, es que ponen énfasis en la distancia de los datos a la línea de regresión utilizando funciones de pérdida, como la función de pérdida $\varepsilon$-sensitiva.
%\begin{equation}
%    |f(\vec{x}) - y|_{\varepsilon} =
%    \left\{
%    \begin{array}{ll}
%        0                               & \mbox{si\,\, }|f(\vec{x}) - y| < \varepsilon\\
%        |f(\vec{x}) - y| - \varepsilon     & eoc
%    \end{array}
%    \right.
%\end{equation}
%donde $\varepsilon > 0$ es un valor escogido con anterioridad.

El algoritmo minimiza el riesgo funcional $||\vec{w}||^2$ a la que se añade una penalización dada por las variables de holgura $\xi$ dependiendo de la distancia del dato hasta la línea de regresión.
%\begin{equation}
%    minimize\, \theta(\vec{w}, \xi) = \frac{1}{2}||\vec{w}||^2 + C\sum_{i = 1}^{l}\xi_{i}
%\end{equation}

%$C$ es un parámetro del modelo para determinar la compensación entre la complejidad del modelo, expresada por $\vec{w}$ y los puntos que quedan fuera de la región descrita por el tubo

La variación de la $v-SVM$ de \cite{Scholkopf2000} consiste en agregar $\varepsilon$ al problema de minimización, ponderando por una variable $v$ que ajusta la contribución de $\varepsilon$ entre $0$ y $1$
%\begin{equation}
%    minimize\, \theta(\vec{w}, \xi) = \frac{1}{2}||\vec{w}||^2 + C\left(lv\varepsilon + \sum_{i = 1}^{l}\xi_{i}\right)
%\end{equation}
%donde $l$ representa la dimensión del vector de datos.
La solución a este problema de minimización para obtener el vector de peso $\vec{w}$ se encuentra mediante el procedimiento estándar para un problema con restricciones de desigualdad en la aplicación de las condiciones de Kuhn-Tuker al problema dual. La principal ventaja del uso del parámetro $v$ es poder controlar el error y el número de \sv\, con un solo parámetro normalizado.

Para resolver el problema de la regresión no lineal basta con cambiar el producto interno entre dos variables independientes $\vec{x_i}\cdot\vec{x_j}$ por una función Kernel $K(\Phi(\vec{x_i})\cdot\Phi(\vec{x_j}))$. Algunas funciones Kernel que se puede utilizar puede ser la función de base radial gaussiano ($RBF$)
%Para resolver el problema de la regresión no lineal basta con cambiar el producto interno entre dos variables independientes $\vec{x_i}\cdot\vec{x_j}$ (ecuación \ref{eq:svm}) por una función Kernel $K(\Phi(\vec{x_i})\cdot\Phi(\vec{x_j}))$. Algunas funciones Kernel que se puede utilizar puede ser la función de base radial gaussiano ($RBF$)
%\begin{equation}
%    K(\vec{x_i}, \vec{x_j}) = \exp\left(\frac{-||\vec{x_i} - \vec{x_j}||^2}{2\sigma^2}\right)
%\end{equation}
%o la función polinomial $K(\vec{x_i}, \vec{x_j}) = (\vec{x_i}, \vec{x_j})^p$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Características de la solución}
La solución propone un análisis teórico mediante la aplicación de minería de datos, donde se estudiarán las capacidades de aprendizaje de los modelos lineales y no lineales basado en los datos que describen la autorregulación cerebral para un problema aplicado a la investigación en las ramas de la biología y medicina. Se utilizarán modelos lineales y no lineales extraídos de la literatura para determinar los modelos que se evaluarán.

\subsection{Propósitos de la solución}
El propósito del presente trabajo es comparar los resultados del aprendizaje de los métodos dinámicos de la autorregulación cerebral en los seres humanos, determinando las características del proceso en función de las bandas de frecuencias y ruido.

\subsection{Alcances o limitaciones de la solución}
Los alcances y limitaciones descritos para el trabajo son los siguientes
\begin{itemize}
	\item El estudio se plantea desde una perspectiva teórica y no precisa de la experimentación con pacientes.

    \item Los datos que se utilizarán son los obtenidos por \cite{Mahony2000} en su publicación, datos que fueron extraídos con el consentimiento de los pacientes y aceptados por el comité respectivo.

	\item Se estudiarán los modelos que describen la denominada autorregulación dinámica.

	\item El ruido presente en las señales será generado de forma sintética.
\end{itemize}
